// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/protobuf/tpu/optimization_parameters.proto

#ifndef PROTOBUF_INCLUDED_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto
#define PROTOBUF_INCLUDED_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3006001
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3006001 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/inlined_string_field.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/generated_enum_reflection.h>
#include <google/protobuf/unknown_field_set.h>
#include <google/protobuf/wrappers.pb.h>
#include "tensorflow/compiler/xla/service/hlo.pb.h"
// @@protoc_insertion_point(includes)
#define PROTOBUF_INTERNAL_EXPORT_protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto 

namespace protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto {
// Internal implementation detail -- do not use these members.
struct TableStruct {
  static const ::google::protobuf::internal::ParseTableField entries[];
  static const ::google::protobuf::internal::AuxillaryParseTableField aux[];
  static const ::google::protobuf::internal::ParseTable schema[28];
  static const ::google::protobuf::internal::FieldMetadata field_metadata[];
  static const ::google::protobuf::internal::SerializationTable serialization_table[];
  static const ::google::protobuf::uint32 offsets[];
};
void AddDescriptors();
}  // namespace protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto
namespace tensorflow {
namespace tpu {
class AdadeltaParameters;
class AdadeltaParametersDefaultTypeInternal;
extern AdadeltaParametersDefaultTypeInternal _AdadeltaParameters_default_instance_;
class AdagradMomentumParameters;
class AdagradMomentumParametersDefaultTypeInternal;
extern AdagradMomentumParametersDefaultTypeInternal _AdagradMomentumParameters_default_instance_;
class AdagradParameters;
class AdagradParametersDefaultTypeInternal;
extern AdagradParametersDefaultTypeInternal _AdagradParameters_default_instance_;
class AdamParameters;
class AdamParametersDefaultTypeInternal;
extern AdamParametersDefaultTypeInternal _AdamParameters_default_instance_;
class AssignParameters;
class AssignParametersDefaultTypeInternal;
extern AssignParametersDefaultTypeInternal _AssignParameters_default_instance_;
class BoundedAdagradParameters;
class BoundedAdagradParametersDefaultTypeInternal;
extern BoundedAdagradParametersDefaultTypeInternal _BoundedAdagradParameters_default_instance_;
class CenteredRmsPropParameters;
class CenteredRmsPropParametersDefaultTypeInternal;
extern CenteredRmsPropParametersDefaultTypeInternal _CenteredRmsPropParameters_default_instance_;
class ClippingLimits;
class ClippingLimitsDefaultTypeInternal;
extern ClippingLimitsDefaultTypeInternal _ClippingLimits_default_instance_;
class DynamicLearningRate;
class DynamicLearningRateDefaultTypeInternal;
extern DynamicLearningRateDefaultTypeInternal _DynamicLearningRate_default_instance_;
class FrequencyEstimatorParameters;
class FrequencyEstimatorParametersDefaultTypeInternal;
extern FrequencyEstimatorParametersDefaultTypeInternal _FrequencyEstimatorParameters_default_instance_;
class FtrlParameters;
class FtrlParametersDefaultTypeInternal;
extern FtrlParametersDefaultTypeInternal _FtrlParameters_default_instance_;
class GradientAccumulationStatus;
class GradientAccumulationStatusDefaultTypeInternal;
extern GradientAccumulationStatusDefaultTypeInternal _GradientAccumulationStatus_default_instance_;
class HotIdReplicationConfiguration;
class HotIdReplicationConfigurationDefaultTypeInternal;
extern HotIdReplicationConfigurationDefaultTypeInternal _HotIdReplicationConfiguration_default_instance_;
class LearningRate;
class LearningRateDefaultTypeInternal;
extern LearningRateDefaultTypeInternal _LearningRate_default_instance_;
class LowDimensionalPackingStatus;
class LowDimensionalPackingStatusDefaultTypeInternal;
extern LowDimensionalPackingStatusDefaultTypeInternal _LowDimensionalPackingStatus_default_instance_;
class MdlAdagradLightParameters;
class MdlAdagradLightParametersDefaultTypeInternal;
extern MdlAdagradLightParametersDefaultTypeInternal _MdlAdagradLightParameters_default_instance_;
class MomentumParameters;
class MomentumParametersDefaultTypeInternal;
extern MomentumParametersDefaultTypeInternal _MomentumParameters_default_instance_;
class OnlineYogiParameters;
class OnlineYogiParametersDefaultTypeInternal;
extern OnlineYogiParametersDefaultTypeInternal _OnlineYogiParameters_default_instance_;
class OptimizationParameters;
class OptimizationParametersDefaultTypeInternal;
extern OptimizationParametersDefaultTypeInternal _OptimizationParameters_default_instance_;
class ProximalAdagradParameters;
class ProximalAdagradParametersDefaultTypeInternal;
extern ProximalAdagradParametersDefaultTypeInternal _ProximalAdagradParameters_default_instance_;
class ProximalYogiParameters;
class ProximalYogiParametersDefaultTypeInternal;
extern ProximalYogiParametersDefaultTypeInternal _ProximalYogiParameters_default_instance_;
class RmsPropParameters;
class RmsPropParametersDefaultTypeInternal;
extern RmsPropParametersDefaultTypeInternal _RmsPropParameters_default_instance_;
class SimulatedQuantization;
class SimulatedQuantizationDefaultTypeInternal;
extern SimulatedQuantizationDefaultTypeInternal _SimulatedQuantization_default_instance_;
class StateVariableSpecification;
class StateVariableSpecificationDefaultTypeInternal;
extern StateVariableSpecificationDefaultTypeInternal _StateVariableSpecification_default_instance_;
class StateVariableSpecification_FillWithConstant;
class StateVariableSpecification_FillWithConstantDefaultTypeInternal;
extern StateVariableSpecification_FillWithConstantDefaultTypeInternal _StateVariableSpecification_FillWithConstant_default_instance_;
class StateVariableSpecification_UserDefined;
class StateVariableSpecification_UserDefinedDefaultTypeInternal;
extern StateVariableSpecification_UserDefinedDefaultTypeInternal _StateVariableSpecification_UserDefined_default_instance_;
class StochasticGradientDescentParameters;
class StochasticGradientDescentParametersDefaultTypeInternal;
extern StochasticGradientDescentParametersDefaultTypeInternal _StochasticGradientDescentParameters_default_instance_;
class UserDefinedProgramParameters;
class UserDefinedProgramParametersDefaultTypeInternal;
extern UserDefinedProgramParametersDefaultTypeInternal _UserDefinedProgramParameters_default_instance_;
}  // namespace tpu
}  // namespace tensorflow
namespace google {
namespace protobuf {
template<> ::tensorflow::tpu::AdadeltaParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::AdadeltaParameters>(Arena*);
template<> ::tensorflow::tpu::AdagradMomentumParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::AdagradMomentumParameters>(Arena*);
template<> ::tensorflow::tpu::AdagradParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::AdagradParameters>(Arena*);
template<> ::tensorflow::tpu::AdamParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::AdamParameters>(Arena*);
template<> ::tensorflow::tpu::AssignParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::AssignParameters>(Arena*);
template<> ::tensorflow::tpu::BoundedAdagradParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::BoundedAdagradParameters>(Arena*);
template<> ::tensorflow::tpu::CenteredRmsPropParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::CenteredRmsPropParameters>(Arena*);
template<> ::tensorflow::tpu::ClippingLimits* Arena::CreateMaybeMessage<::tensorflow::tpu::ClippingLimits>(Arena*);
template<> ::tensorflow::tpu::DynamicLearningRate* Arena::CreateMaybeMessage<::tensorflow::tpu::DynamicLearningRate>(Arena*);
template<> ::tensorflow::tpu::FrequencyEstimatorParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::FrequencyEstimatorParameters>(Arena*);
template<> ::tensorflow::tpu::FtrlParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::FtrlParameters>(Arena*);
template<> ::tensorflow::tpu::GradientAccumulationStatus* Arena::CreateMaybeMessage<::tensorflow::tpu::GradientAccumulationStatus>(Arena*);
template<> ::tensorflow::tpu::HotIdReplicationConfiguration* Arena::CreateMaybeMessage<::tensorflow::tpu::HotIdReplicationConfiguration>(Arena*);
template<> ::tensorflow::tpu::LearningRate* Arena::CreateMaybeMessage<::tensorflow::tpu::LearningRate>(Arena*);
template<> ::tensorflow::tpu::LowDimensionalPackingStatus* Arena::CreateMaybeMessage<::tensorflow::tpu::LowDimensionalPackingStatus>(Arena*);
template<> ::tensorflow::tpu::MdlAdagradLightParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::MdlAdagradLightParameters>(Arena*);
template<> ::tensorflow::tpu::MomentumParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::MomentumParameters>(Arena*);
template<> ::tensorflow::tpu::OnlineYogiParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::OnlineYogiParameters>(Arena*);
template<> ::tensorflow::tpu::OptimizationParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::OptimizationParameters>(Arena*);
template<> ::tensorflow::tpu::ProximalAdagradParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::ProximalAdagradParameters>(Arena*);
template<> ::tensorflow::tpu::ProximalYogiParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::ProximalYogiParameters>(Arena*);
template<> ::tensorflow::tpu::RmsPropParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::RmsPropParameters>(Arena*);
template<> ::tensorflow::tpu::SimulatedQuantization* Arena::CreateMaybeMessage<::tensorflow::tpu::SimulatedQuantization>(Arena*);
template<> ::tensorflow::tpu::StateVariableSpecification* Arena::CreateMaybeMessage<::tensorflow::tpu::StateVariableSpecification>(Arena*);
template<> ::tensorflow::tpu::StateVariableSpecification_FillWithConstant* Arena::CreateMaybeMessage<::tensorflow::tpu::StateVariableSpecification_FillWithConstant>(Arena*);
template<> ::tensorflow::tpu::StateVariableSpecification_UserDefined* Arena::CreateMaybeMessage<::tensorflow::tpu::StateVariableSpecification_UserDefined>(Arena*);
template<> ::tensorflow::tpu::StochasticGradientDescentParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::StochasticGradientDescentParameters>(Arena*);
template<> ::tensorflow::tpu::UserDefinedProgramParameters* Arena::CreateMaybeMessage<::tensorflow::tpu::UserDefinedProgramParameters>(Arena*);
}  // namespace protobuf
}  // namespace google
namespace tensorflow {
namespace tpu {

enum GradientAccumulationStatus_Status {
  GradientAccumulationStatus_Status_UNSPECIFIED = 0,
  GradientAccumulationStatus_Status_ENABLED = 1,
  GradientAccumulationStatus_Status_DISABLED = 2,
  GradientAccumulationStatus_Status_GradientAccumulationStatus_Status_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  GradientAccumulationStatus_Status_GradientAccumulationStatus_Status_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool GradientAccumulationStatus_Status_IsValid(int value);
const GradientAccumulationStatus_Status GradientAccumulationStatus_Status_Status_MIN = GradientAccumulationStatus_Status_UNSPECIFIED;
const GradientAccumulationStatus_Status GradientAccumulationStatus_Status_Status_MAX = GradientAccumulationStatus_Status_DISABLED;
const int GradientAccumulationStatus_Status_Status_ARRAYSIZE = GradientAccumulationStatus_Status_Status_MAX + 1;

const ::google::protobuf::EnumDescriptor* GradientAccumulationStatus_Status_descriptor();
inline const ::std::string& GradientAccumulationStatus_Status_Name(GradientAccumulationStatus_Status value) {
  return ::google::protobuf::internal::NameOfEnum(
    GradientAccumulationStatus_Status_descriptor(), value);
}
inline bool GradientAccumulationStatus_Status_Parse(
    const ::std::string& name, GradientAccumulationStatus_Status* value) {
  return ::google::protobuf::internal::ParseNamedEnum<GradientAccumulationStatus_Status>(
    GradientAccumulationStatus_Status_descriptor(), name, value);
}
enum LowDimensionalPackingStatus_Status {
  LowDimensionalPackingStatus_Status_UNSPECIFIED = 0,
  LowDimensionalPackingStatus_Status_ENABLED = 1,
  LowDimensionalPackingStatus_Status_DISABLED = 2,
  LowDimensionalPackingStatus_Status_LowDimensionalPackingStatus_Status_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  LowDimensionalPackingStatus_Status_LowDimensionalPackingStatus_Status_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool LowDimensionalPackingStatus_Status_IsValid(int value);
const LowDimensionalPackingStatus_Status LowDimensionalPackingStatus_Status_Status_MIN = LowDimensionalPackingStatus_Status_UNSPECIFIED;
const LowDimensionalPackingStatus_Status LowDimensionalPackingStatus_Status_Status_MAX = LowDimensionalPackingStatus_Status_DISABLED;
const int LowDimensionalPackingStatus_Status_Status_ARRAYSIZE = LowDimensionalPackingStatus_Status_Status_MAX + 1;

const ::google::protobuf::EnumDescriptor* LowDimensionalPackingStatus_Status_descriptor();
inline const ::std::string& LowDimensionalPackingStatus_Status_Name(LowDimensionalPackingStatus_Status value) {
  return ::google::protobuf::internal::NameOfEnum(
    LowDimensionalPackingStatus_Status_descriptor(), value);
}
inline bool LowDimensionalPackingStatus_Status_Parse(
    const ::std::string& name, LowDimensionalPackingStatus_Status* value) {
  return ::google::protobuf::internal::ParseNamedEnum<LowDimensionalPackingStatus_Status>(
    LowDimensionalPackingStatus_Status_descriptor(), name, value);
}
enum HotIdReplicationConfiguration_Status {
  HotIdReplicationConfiguration_Status_UNSPECIFIED = 0,
  HotIdReplicationConfiguration_Status_ENABLED = 1,
  HotIdReplicationConfiguration_Status_DISABLED = 2,
  HotIdReplicationConfiguration_Status_HotIdReplicationConfiguration_Status_INT_MIN_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32min,
  HotIdReplicationConfiguration_Status_HotIdReplicationConfiguration_Status_INT_MAX_SENTINEL_DO_NOT_USE_ = ::google::protobuf::kint32max
};
bool HotIdReplicationConfiguration_Status_IsValid(int value);
const HotIdReplicationConfiguration_Status HotIdReplicationConfiguration_Status_Status_MIN = HotIdReplicationConfiguration_Status_UNSPECIFIED;
const HotIdReplicationConfiguration_Status HotIdReplicationConfiguration_Status_Status_MAX = HotIdReplicationConfiguration_Status_DISABLED;
const int HotIdReplicationConfiguration_Status_Status_ARRAYSIZE = HotIdReplicationConfiguration_Status_Status_MAX + 1;

const ::google::protobuf::EnumDescriptor* HotIdReplicationConfiguration_Status_descriptor();
inline const ::std::string& HotIdReplicationConfiguration_Status_Name(HotIdReplicationConfiguration_Status value) {
  return ::google::protobuf::internal::NameOfEnum(
    HotIdReplicationConfiguration_Status_descriptor(), value);
}
inline bool HotIdReplicationConfiguration_Status_Parse(
    const ::std::string& name, HotIdReplicationConfiguration_Status* value) {
  return ::google::protobuf::internal::ParseNamedEnum<HotIdReplicationConfiguration_Status>(
    HotIdReplicationConfiguration_Status_descriptor(), name, value);
}
// ===================================================================

class ClippingLimits : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.ClippingLimits) */ {
 public:
  ClippingLimits();
  virtual ~ClippingLimits();

  ClippingLimits(const ClippingLimits& from);

  inline ClippingLimits& operator=(const ClippingLimits& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ClippingLimits(ClippingLimits&& from) noexcept
    : ClippingLimits() {
    *this = ::std::move(from);
  }

  inline ClippingLimits& operator=(ClippingLimits&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ClippingLimits& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ClippingLimits* internal_default_instance() {
    return reinterpret_cast<const ClippingLimits*>(
               &_ClippingLimits_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  void Swap(ClippingLimits* other);
  friend void swap(ClippingLimits& a, ClippingLimits& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ClippingLimits* New() const final {
    return CreateMaybeMessage<ClippingLimits>(NULL);
  }

  ClippingLimits* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ClippingLimits>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ClippingLimits& from);
  void MergeFrom(const ClippingLimits& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ClippingLimits* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .google.protobuf.FloatValue lower = 1;
  bool has_lower() const;
  void clear_lower();
  static const int kLowerFieldNumber = 1;
  private:
  const ::google::protobuf::FloatValue& _internal_lower() const;
  public:
  const ::google::protobuf::FloatValue& lower() const;
  ::google::protobuf::FloatValue* release_lower();
  ::google::protobuf::FloatValue* mutable_lower();
  void set_allocated_lower(::google::protobuf::FloatValue* lower);

  // .google.protobuf.FloatValue upper = 2;
  bool has_upper() const;
  void clear_upper();
  static const int kUpperFieldNumber = 2;
  private:
  const ::google::protobuf::FloatValue& _internal_upper() const;
  public:
  const ::google::protobuf::FloatValue& upper() const;
  ::google::protobuf::FloatValue* release_upper();
  ::google::protobuf::FloatValue* mutable_upper();
  void set_allocated_upper(::google::protobuf::FloatValue* upper);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.ClippingLimits)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::FloatValue* lower_;
  ::google::protobuf::FloatValue* upper_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SimulatedQuantization : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.SimulatedQuantization) */ {
 public:
  SimulatedQuantization();
  virtual ~SimulatedQuantization();

  SimulatedQuantization(const SimulatedQuantization& from);

  inline SimulatedQuantization& operator=(const SimulatedQuantization& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SimulatedQuantization(SimulatedQuantization&& from) noexcept
    : SimulatedQuantization() {
    *this = ::std::move(from);
  }

  inline SimulatedQuantization& operator=(SimulatedQuantization&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const SimulatedQuantization& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SimulatedQuantization* internal_default_instance() {
    return reinterpret_cast<const SimulatedQuantization*>(
               &_SimulatedQuantization_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  void Swap(SimulatedQuantization* other);
  friend void swap(SimulatedQuantization& a, SimulatedQuantization& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SimulatedQuantization* New() const final {
    return CreateMaybeMessage<SimulatedQuantization>(NULL);
  }

  SimulatedQuantization* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SimulatedQuantization>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SimulatedQuantization& from);
  void MergeFrom(const SimulatedQuantization& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SimulatedQuantization* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .tensorflow.tpu.ClippingLimits clipping_limits = 2;
  bool has_clipping_limits() const;
  void clear_clipping_limits();
  static const int kClippingLimitsFieldNumber = 2;
  private:
  const ::tensorflow::tpu::ClippingLimits& _internal_clipping_limits() const;
  public:
  const ::tensorflow::tpu::ClippingLimits& clipping_limits() const;
  ::tensorflow::tpu::ClippingLimits* release_clipping_limits();
  ::tensorflow::tpu::ClippingLimits* mutable_clipping_limits();
  void set_allocated_clipping_limits(::tensorflow::tpu::ClippingLimits* clipping_limits);

  // bool enabled = 1;
  void clear_enabled();
  static const int kEnabledFieldNumber = 1;
  bool enabled() const;
  void set_enabled(bool value);

  // int32 num_buckets = 3;
  void clear_num_buckets();
  static const int kNumBucketsFieldNumber = 3;
  ::google::protobuf::int32 num_buckets() const;
  void set_num_buckets(::google::protobuf::int32 value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.SimulatedQuantization)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::tensorflow::tpu::ClippingLimits* clipping_limits_;
  bool enabled_;
  ::google::protobuf::int32 num_buckets_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class DynamicLearningRate : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.DynamicLearningRate) */ {
 public:
  DynamicLearningRate();
  virtual ~DynamicLearningRate();

  DynamicLearningRate(const DynamicLearningRate& from);

  inline DynamicLearningRate& operator=(const DynamicLearningRate& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  DynamicLearningRate(DynamicLearningRate&& from) noexcept
    : DynamicLearningRate() {
    *this = ::std::move(from);
  }

  inline DynamicLearningRate& operator=(DynamicLearningRate&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const DynamicLearningRate& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const DynamicLearningRate* internal_default_instance() {
    return reinterpret_cast<const DynamicLearningRate*>(
               &_DynamicLearningRate_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  void Swap(DynamicLearningRate* other);
  friend void swap(DynamicLearningRate& a, DynamicLearningRate& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline DynamicLearningRate* New() const final {
    return CreateMaybeMessage<DynamicLearningRate>(NULL);
  }

  DynamicLearningRate* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<DynamicLearningRate>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const DynamicLearningRate& from);
  void MergeFrom(const DynamicLearningRate& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(DynamicLearningRate* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // int32 tag = 1;
  void clear_tag();
  static const int kTagFieldNumber = 1;
  ::google::protobuf::int32 tag() const;
  void set_tag(::google::protobuf::int32 value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.DynamicLearningRate)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::int32 tag_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LearningRate : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.LearningRate) */ {
 public:
  LearningRate();
  virtual ~LearningRate();

  LearningRate(const LearningRate& from);

  inline LearningRate& operator=(const LearningRate& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  LearningRate(LearningRate&& from) noexcept
    : LearningRate() {
    *this = ::std::move(from);
  }

  inline LearningRate& operator=(LearningRate&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const LearningRate& default_instance();

  enum LearningRateCase {
    kConstant = 1,
    kDynamic = 2,
    LEARNING_RATE_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const LearningRate* internal_default_instance() {
    return reinterpret_cast<const LearningRate*>(
               &_LearningRate_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  void Swap(LearningRate* other);
  friend void swap(LearningRate& a, LearningRate& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline LearningRate* New() const final {
    return CreateMaybeMessage<LearningRate>(NULL);
  }

  LearningRate* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<LearningRate>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const LearningRate& from);
  void MergeFrom(const LearningRate& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(LearningRate* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float constant = 1;
  private:
  bool has_constant() const;
  public:
  void clear_constant();
  static const int kConstantFieldNumber = 1;
  float constant() const;
  void set_constant(float value);

  // .tensorflow.tpu.DynamicLearningRate dynamic = 2;
  bool has_dynamic() const;
  void clear_dynamic();
  static const int kDynamicFieldNumber = 2;
  private:
  const ::tensorflow::tpu::DynamicLearningRate& _internal_dynamic() const;
  public:
  const ::tensorflow::tpu::DynamicLearningRate& dynamic() const;
  ::tensorflow::tpu::DynamicLearningRate* release_dynamic();
  ::tensorflow::tpu::DynamicLearningRate* mutable_dynamic();
  void set_allocated_dynamic(::tensorflow::tpu::DynamicLearningRate* dynamic);

  void clear_learning_rate();
  LearningRateCase learning_rate_case() const;
  // @@protoc_insertion_point(class_scope:tensorflow.tpu.LearningRate)
 private:
  void set_has_constant();
  void set_has_dynamic();

  inline bool has_learning_rate() const;
  inline void clear_has_learning_rate();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  union LearningRateUnion {
    LearningRateUnion() {}
    float constant_;
    ::tensorflow::tpu::DynamicLearningRate* dynamic_;
  } learning_rate_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AdagradParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.AdagradParameters) */ {
 public:
  AdagradParameters();
  virtual ~AdagradParameters();

  AdagradParameters(const AdagradParameters& from);

  inline AdagradParameters& operator=(const AdagradParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  AdagradParameters(AdagradParameters&& from) noexcept
    : AdagradParameters() {
    *this = ::std::move(from);
  }

  inline AdagradParameters& operator=(AdagradParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const AdagradParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const AdagradParameters* internal_default_instance() {
    return reinterpret_cast<const AdagradParameters*>(
               &_AdagradParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  void Swap(AdagradParameters* other);
  friend void swap(AdagradParameters& a, AdagradParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline AdagradParameters* New() const final {
    return CreateMaybeMessage<AdagradParameters>(NULL);
  }

  AdagradParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<AdagradParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const AdagradParameters& from);
  void MergeFrom(const AdagradParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(AdagradParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.AdagradParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AdagradMomentumParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.AdagradMomentumParameters) */ {
 public:
  AdagradMomentumParameters();
  virtual ~AdagradMomentumParameters();

  AdagradMomentumParameters(const AdagradMomentumParameters& from);

  inline AdagradMomentumParameters& operator=(const AdagradMomentumParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  AdagradMomentumParameters(AdagradMomentumParameters&& from) noexcept
    : AdagradMomentumParameters() {
    *this = ::std::move(from);
  }

  inline AdagradMomentumParameters& operator=(AdagradMomentumParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const AdagradMomentumParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const AdagradMomentumParameters* internal_default_instance() {
    return reinterpret_cast<const AdagradMomentumParameters*>(
               &_AdagradMomentumParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  void Swap(AdagradMomentumParameters* other);
  friend void swap(AdagradMomentumParameters& a, AdagradMomentumParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline AdagradMomentumParameters* New() const final {
    return CreateMaybeMessage<AdagradMomentumParameters>(NULL);
  }

  AdagradMomentumParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<AdagradMomentumParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const AdagradMomentumParameters& from);
  void MergeFrom(const AdagradMomentumParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(AdagradMomentumParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float momentum = 1;
  void clear_momentum();
  static const int kMomentumFieldNumber = 1;
  float momentum() const;
  void set_momentum(float value);

  // bool use_nesterov = 2;
  void clear_use_nesterov();
  static const int kUseNesterovFieldNumber = 2;
  bool use_nesterov() const;
  void set_use_nesterov(bool value);

  // float exponent = 3;
  void clear_exponent();
  static const int kExponentFieldNumber = 3;
  float exponent() const;
  void set_exponent(float value);

  // float beta2 = 4;
  void clear_beta2();
  static const int kBeta2FieldNumber = 4;
  float beta2() const;
  void set_beta2(float value);

  // float epsilon = 5;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 5;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.AdagradMomentumParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float momentum_;
  bool use_nesterov_;
  float exponent_;
  float beta2_;
  float epsilon_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BoundedAdagradParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.BoundedAdagradParameters) */ {
 public:
  BoundedAdagradParameters();
  virtual ~BoundedAdagradParameters();

  BoundedAdagradParameters(const BoundedAdagradParameters& from);

  inline BoundedAdagradParameters& operator=(const BoundedAdagradParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BoundedAdagradParameters(BoundedAdagradParameters&& from) noexcept
    : BoundedAdagradParameters() {
    *this = ::std::move(from);
  }

  inline BoundedAdagradParameters& operator=(BoundedAdagradParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const BoundedAdagradParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BoundedAdagradParameters* internal_default_instance() {
    return reinterpret_cast<const BoundedAdagradParameters*>(
               &_BoundedAdagradParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  void Swap(BoundedAdagradParameters* other);
  friend void swap(BoundedAdagradParameters& a, BoundedAdagradParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BoundedAdagradParameters* New() const final {
    return CreateMaybeMessage<BoundedAdagradParameters>(NULL);
  }

  BoundedAdagradParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BoundedAdagradParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BoundedAdagradParameters& from);
  void MergeFrom(const BoundedAdagradParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BoundedAdagradParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // bool update_accumulator_first = 1;
  void clear_update_accumulator_first();
  static const int kUpdateAccumulatorFirstFieldNumber = 1;
  bool update_accumulator_first() const;
  void set_update_accumulator_first(bool value);

  // float max_var_update = 2;
  void clear_max_var_update();
  static const int kMaxVarUpdateFieldNumber = 2;
  float max_var_update() const;
  void set_max_var_update(float value);

  // float max_accumulator = 3;
  void clear_max_accumulator();
  static const int kMaxAccumulatorFieldNumber = 3;
  float max_accumulator() const;
  void set_max_accumulator(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.BoundedAdagradParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  bool update_accumulator_first_;
  float max_var_update_;
  float max_accumulator_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StochasticGradientDescentParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.StochasticGradientDescentParameters) */ {
 public:
  StochasticGradientDescentParameters();
  virtual ~StochasticGradientDescentParameters();

  StochasticGradientDescentParameters(const StochasticGradientDescentParameters& from);

  inline StochasticGradientDescentParameters& operator=(const StochasticGradientDescentParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StochasticGradientDescentParameters(StochasticGradientDescentParameters&& from) noexcept
    : StochasticGradientDescentParameters() {
    *this = ::std::move(from);
  }

  inline StochasticGradientDescentParameters& operator=(StochasticGradientDescentParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const StochasticGradientDescentParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StochasticGradientDescentParameters* internal_default_instance() {
    return reinterpret_cast<const StochasticGradientDescentParameters*>(
               &_StochasticGradientDescentParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  void Swap(StochasticGradientDescentParameters* other);
  friend void swap(StochasticGradientDescentParameters& a, StochasticGradientDescentParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StochasticGradientDescentParameters* New() const final {
    return CreateMaybeMessage<StochasticGradientDescentParameters>(NULL);
  }

  StochasticGradientDescentParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StochasticGradientDescentParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StochasticGradientDescentParameters& from);
  void MergeFrom(const StochasticGradientDescentParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StochasticGradientDescentParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.StochasticGradientDescentParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FtrlParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.FtrlParameters) */ {
 public:
  FtrlParameters();
  virtual ~FtrlParameters();

  FtrlParameters(const FtrlParameters& from);

  inline FtrlParameters& operator=(const FtrlParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  FtrlParameters(FtrlParameters&& from) noexcept
    : FtrlParameters() {
    *this = ::std::move(from);
  }

  inline FtrlParameters& operator=(FtrlParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const FtrlParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const FtrlParameters* internal_default_instance() {
    return reinterpret_cast<const FtrlParameters*>(
               &_FtrlParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    8;

  void Swap(FtrlParameters* other);
  friend void swap(FtrlParameters& a, FtrlParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline FtrlParameters* New() const final {
    return CreateMaybeMessage<FtrlParameters>(NULL);
  }

  FtrlParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<FtrlParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const FtrlParameters& from);
  void MergeFrom(const FtrlParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(FtrlParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float l1 = 1;
  void clear_l1();
  static const int kL1FieldNumber = 1;
  float l1() const;
  void set_l1(float value);

  // float l2 = 2;
  void clear_l2();
  static const int kL2FieldNumber = 2;
  float l2() const;
  void set_l2(float value);

  // float lr_power = 3;
  void clear_lr_power();
  static const int kLrPowerFieldNumber = 3;
  float lr_power() const;
  void set_lr_power(float value);

  // float beta = 7;
  void clear_beta();
  static const int kBetaFieldNumber = 7;
  float beta() const;
  void set_beta(float value);

  // bool multiply_linear_by_lr = 6;
  void clear_multiply_linear_by_lr();
  static const int kMultiplyLinearByLrFieldNumber = 6;
  bool multiply_linear_by_lr() const;
  void set_multiply_linear_by_lr(bool value);

  // bool allow_zero_accumulator = 8 [deprecated = true];
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void clear_allow_zero_accumulator();
  GOOGLE_PROTOBUF_DEPRECATED_ATTR static const int kAllowZeroAccumulatorFieldNumber = 8;
  GOOGLE_PROTOBUF_DEPRECATED_ATTR bool allow_zero_accumulator() const;
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void set_allow_zero_accumulator(bool value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.FtrlParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float l1_;
  float l2_;
  float lr_power_;
  float beta_;
  bool multiply_linear_by_lr_;
  bool allow_zero_accumulator_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AdamParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.AdamParameters) */ {
 public:
  AdamParameters();
  virtual ~AdamParameters();

  AdamParameters(const AdamParameters& from);

  inline AdamParameters& operator=(const AdamParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  AdamParameters(AdamParameters&& from) noexcept
    : AdamParameters() {
    *this = ::std::move(from);
  }

  inline AdamParameters& operator=(AdamParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const AdamParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const AdamParameters* internal_default_instance() {
    return reinterpret_cast<const AdamParameters*>(
               &_AdamParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    9;

  void Swap(AdamParameters* other);
  friend void swap(AdamParameters& a, AdamParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline AdamParameters* New() const final {
    return CreateMaybeMessage<AdamParameters>(NULL);
  }

  AdamParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<AdamParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const AdamParameters& from);
  void MergeFrom(const AdamParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(AdamParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float beta1 = 3;
  void clear_beta1();
  static const int kBeta1FieldNumber = 3;
  float beta1() const;
  void set_beta1(float value);

  // float beta2 = 4;
  void clear_beta2();
  static const int kBeta2FieldNumber = 4;
  float beta2() const;
  void set_beta2(float value);

  // float epsilon = 5;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 5;
  float epsilon() const;
  void set_epsilon(float value);

  // bool use_non_lazy_adam = 8;
  void clear_use_non_lazy_adam();
  static const int kUseNonLazyAdamFieldNumber = 8;
  bool use_non_lazy_adam() const;
  void set_use_non_lazy_adam(bool value);

  // bool use_sum_inside_sqrt = 10;
  void clear_use_sum_inside_sqrt();
  static const int kUseSumInsideSqrtFieldNumber = 10;
  bool use_sum_inside_sqrt() const;
  void set_use_sum_inside_sqrt(bool value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.AdamParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float beta1_;
  float beta2_;
  float epsilon_;
  bool use_non_lazy_adam_;
  bool use_sum_inside_sqrt_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MomentumParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.MomentumParameters) */ {
 public:
  MomentumParameters();
  virtual ~MomentumParameters();

  MomentumParameters(const MomentumParameters& from);

  inline MomentumParameters& operator=(const MomentumParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  MomentumParameters(MomentumParameters&& from) noexcept
    : MomentumParameters() {
    *this = ::std::move(from);
  }

  inline MomentumParameters& operator=(MomentumParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const MomentumParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const MomentumParameters* internal_default_instance() {
    return reinterpret_cast<const MomentumParameters*>(
               &_MomentumParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    10;

  void Swap(MomentumParameters* other);
  friend void swap(MomentumParameters& a, MomentumParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline MomentumParameters* New() const final {
    return CreateMaybeMessage<MomentumParameters>(NULL);
  }

  MomentumParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<MomentumParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const MomentumParameters& from);
  void MergeFrom(const MomentumParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(MomentumParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float momentum = 1;
  void clear_momentum();
  static const int kMomentumFieldNumber = 1;
  float momentum() const;
  void set_momentum(float value);

  // bool use_nesterov = 2;
  void clear_use_nesterov();
  static const int kUseNesterovFieldNumber = 2;
  bool use_nesterov() const;
  void set_use_nesterov(bool value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.MomentumParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float momentum_;
  bool use_nesterov_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RmsPropParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.RmsPropParameters) */ {
 public:
  RmsPropParameters();
  virtual ~RmsPropParameters();

  RmsPropParameters(const RmsPropParameters& from);

  inline RmsPropParameters& operator=(const RmsPropParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  RmsPropParameters(RmsPropParameters&& from) noexcept
    : RmsPropParameters() {
    *this = ::std::move(from);
  }

  inline RmsPropParameters& operator=(RmsPropParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const RmsPropParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const RmsPropParameters* internal_default_instance() {
    return reinterpret_cast<const RmsPropParameters*>(
               &_RmsPropParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    11;

  void Swap(RmsPropParameters* other);
  friend void swap(RmsPropParameters& a, RmsPropParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline RmsPropParameters* New() const final {
    return CreateMaybeMessage<RmsPropParameters>(NULL);
  }

  RmsPropParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<RmsPropParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const RmsPropParameters& from);
  void MergeFrom(const RmsPropParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(RmsPropParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float rho = 1;
  void clear_rho();
  static const int kRhoFieldNumber = 1;
  float rho() const;
  void set_rho(float value);

  // float momentum = 2;
  void clear_momentum();
  static const int kMomentumFieldNumber = 2;
  float momentum() const;
  void set_momentum(float value);

  // float epsilon = 3;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 3;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.RmsPropParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float rho_;
  float momentum_;
  float epsilon_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CenteredRmsPropParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.CenteredRmsPropParameters) */ {
 public:
  CenteredRmsPropParameters();
  virtual ~CenteredRmsPropParameters();

  CenteredRmsPropParameters(const CenteredRmsPropParameters& from);

  inline CenteredRmsPropParameters& operator=(const CenteredRmsPropParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  CenteredRmsPropParameters(CenteredRmsPropParameters&& from) noexcept
    : CenteredRmsPropParameters() {
    *this = ::std::move(from);
  }

  inline CenteredRmsPropParameters& operator=(CenteredRmsPropParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const CenteredRmsPropParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const CenteredRmsPropParameters* internal_default_instance() {
    return reinterpret_cast<const CenteredRmsPropParameters*>(
               &_CenteredRmsPropParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    12;

  void Swap(CenteredRmsPropParameters* other);
  friend void swap(CenteredRmsPropParameters& a, CenteredRmsPropParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline CenteredRmsPropParameters* New() const final {
    return CreateMaybeMessage<CenteredRmsPropParameters>(NULL);
  }

  CenteredRmsPropParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<CenteredRmsPropParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const CenteredRmsPropParameters& from);
  void MergeFrom(const CenteredRmsPropParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CenteredRmsPropParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float rho = 1;
  void clear_rho();
  static const int kRhoFieldNumber = 1;
  float rho() const;
  void set_rho(float value);

  // float momentum = 2;
  void clear_momentum();
  static const int kMomentumFieldNumber = 2;
  float momentum() const;
  void set_momentum(float value);

  // float epsilon = 3;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 3;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.CenteredRmsPropParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float rho_;
  float momentum_;
  float epsilon_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class MdlAdagradLightParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.MdlAdagradLightParameters) */ {
 public:
  MdlAdagradLightParameters();
  virtual ~MdlAdagradLightParameters();

  MdlAdagradLightParameters(const MdlAdagradLightParameters& from);

  inline MdlAdagradLightParameters& operator=(const MdlAdagradLightParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  MdlAdagradLightParameters(MdlAdagradLightParameters&& from) noexcept
    : MdlAdagradLightParameters() {
    *this = ::std::move(from);
  }

  inline MdlAdagradLightParameters& operator=(MdlAdagradLightParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const MdlAdagradLightParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const MdlAdagradLightParameters* internal_default_instance() {
    return reinterpret_cast<const MdlAdagradLightParameters*>(
               &_MdlAdagradLightParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    13;

  void Swap(MdlAdagradLightParameters* other);
  friend void swap(MdlAdagradLightParameters& a, MdlAdagradLightParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline MdlAdagradLightParameters* New() const final {
    return CreateMaybeMessage<MdlAdagradLightParameters>(NULL);
  }

  MdlAdagradLightParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<MdlAdagradLightParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const MdlAdagradLightParameters& from);
  void MergeFrom(const MdlAdagradLightParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(MdlAdagradLightParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float l2 = 1;
  void clear_l2();
  static const int kL2FieldNumber = 1;
  float l2() const;
  void set_l2(float value);

  // float lr_power = 2;
  void clear_lr_power();
  static const int kLrPowerFieldNumber = 2;
  float lr_power() const;
  void set_lr_power(float value);

  // float min_servable_mdl_benefit = 3;
  void clear_min_servable_mdl_benefit();
  static const int kMinServableMdlBenefitFieldNumber = 3;
  float min_servable_mdl_benefit() const;
  void set_min_servable_mdl_benefit(float value);

  // float mdl_mix_in_margin = 4;
  void clear_mdl_mix_in_margin();
  static const int kMdlMixInMarginFieldNumber = 4;
  float mdl_mix_in_margin() const;
  void set_mdl_mix_in_margin(float value);

  // float mdl_benefit_rampup_coeff = 5;
  void clear_mdl_benefit_rampup_coeff();
  static const int kMdlBenefitRampupCoeffFieldNumber = 5;
  float mdl_benefit_rampup_coeff() const;
  void set_mdl_benefit_rampup_coeff(float value);

  // float mdl_min_weight = 6;
  void clear_mdl_min_weight();
  static const int kMdlMinWeightFieldNumber = 6;
  float mdl_min_weight() const;
  void set_mdl_min_weight(float value);

  // float benefit_revisit_scale = 7;
  void clear_benefit_revisit_scale();
  static const int kBenefitRevisitScaleFieldNumber = 7;
  float benefit_revisit_scale() const;
  void set_benefit_revisit_scale(float value);

  // float max_event_benefit = 8;
  void clear_max_event_benefit();
  static const int kMaxEventBenefitFieldNumber = 8;
  float max_event_benefit() const;
  void set_max_event_benefit(float value);

  // float max_total_benefit = 9;
  void clear_max_total_benefit();
  static const int kMaxTotalBenefitFieldNumber = 9;
  float max_total_benefit() const;
  void set_max_total_benefit(float value);

  // float mdl_hard_limit = 10;
  void clear_mdl_hard_limit();
  static const int kMdlHardLimitFieldNumber = 10;
  float mdl_hard_limit() const;
  void set_mdl_hard_limit(float value);

  // bool hard_limit_min_benefit = 11;
  void clear_hard_limit_min_benefit();
  static const int kHardLimitMinBenefitFieldNumber = 11;
  bool hard_limit_min_benefit() const;
  void set_hard_limit_min_benefit(bool value);

  // bool mdl_regularize = 12;
  void clear_mdl_regularize();
  static const int kMdlRegularizeFieldNumber = 12;
  bool mdl_regularize() const;
  void set_mdl_regularize(bool value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.MdlAdagradLightParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float l2_;
  float lr_power_;
  float min_servable_mdl_benefit_;
  float mdl_mix_in_margin_;
  float mdl_benefit_rampup_coeff_;
  float mdl_min_weight_;
  float benefit_revisit_scale_;
  float max_event_benefit_;
  float max_total_benefit_;
  float mdl_hard_limit_;
  bool hard_limit_min_benefit_;
  bool mdl_regularize_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AdadeltaParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.AdadeltaParameters) */ {
 public:
  AdadeltaParameters();
  virtual ~AdadeltaParameters();

  AdadeltaParameters(const AdadeltaParameters& from);

  inline AdadeltaParameters& operator=(const AdadeltaParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  AdadeltaParameters(AdadeltaParameters&& from) noexcept
    : AdadeltaParameters() {
    *this = ::std::move(from);
  }

  inline AdadeltaParameters& operator=(AdadeltaParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const AdadeltaParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const AdadeltaParameters* internal_default_instance() {
    return reinterpret_cast<const AdadeltaParameters*>(
               &_AdadeltaParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    14;

  void Swap(AdadeltaParameters* other);
  friend void swap(AdadeltaParameters& a, AdadeltaParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline AdadeltaParameters* New() const final {
    return CreateMaybeMessage<AdadeltaParameters>(NULL);
  }

  AdadeltaParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<AdadeltaParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const AdadeltaParameters& from);
  void MergeFrom(const AdadeltaParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(AdadeltaParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float rho = 1;
  void clear_rho();
  static const int kRhoFieldNumber = 1;
  float rho() const;
  void set_rho(float value);

  // float epsilon = 2;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 2;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.AdadeltaParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float rho_;
  float epsilon_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ProximalAdagradParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.ProximalAdagradParameters) */ {
 public:
  ProximalAdagradParameters();
  virtual ~ProximalAdagradParameters();

  ProximalAdagradParameters(const ProximalAdagradParameters& from);

  inline ProximalAdagradParameters& operator=(const ProximalAdagradParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ProximalAdagradParameters(ProximalAdagradParameters&& from) noexcept
    : ProximalAdagradParameters() {
    *this = ::std::move(from);
  }

  inline ProximalAdagradParameters& operator=(ProximalAdagradParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ProximalAdagradParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ProximalAdagradParameters* internal_default_instance() {
    return reinterpret_cast<const ProximalAdagradParameters*>(
               &_ProximalAdagradParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    15;

  void Swap(ProximalAdagradParameters* other);
  friend void swap(ProximalAdagradParameters& a, ProximalAdagradParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ProximalAdagradParameters* New() const final {
    return CreateMaybeMessage<ProximalAdagradParameters>(NULL);
  }

  ProximalAdagradParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ProximalAdagradParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ProximalAdagradParameters& from);
  void MergeFrom(const ProximalAdagradParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ProximalAdagradParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float l1 = 1;
  void clear_l1();
  static const int kL1FieldNumber = 1;
  float l1() const;
  void set_l1(float value);

  // float l2 = 2;
  void clear_l2();
  static const int kL2FieldNumber = 2;
  float l2() const;
  void set_l2(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.ProximalAdagradParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float l1_;
  float l2_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class OnlineYogiParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.OnlineYogiParameters) */ {
 public:
  OnlineYogiParameters();
  virtual ~OnlineYogiParameters();

  OnlineYogiParameters(const OnlineYogiParameters& from);

  inline OnlineYogiParameters& operator=(const OnlineYogiParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  OnlineYogiParameters(OnlineYogiParameters&& from) noexcept
    : OnlineYogiParameters() {
    *this = ::std::move(from);
  }

  inline OnlineYogiParameters& operator=(OnlineYogiParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const OnlineYogiParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const OnlineYogiParameters* internal_default_instance() {
    return reinterpret_cast<const OnlineYogiParameters*>(
               &_OnlineYogiParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    16;

  void Swap(OnlineYogiParameters* other);
  friend void swap(OnlineYogiParameters& a, OnlineYogiParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline OnlineYogiParameters* New() const final {
    return CreateMaybeMessage<OnlineYogiParameters>(NULL);
  }

  OnlineYogiParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<OnlineYogiParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const OnlineYogiParameters& from);
  void MergeFrom(const OnlineYogiParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(OnlineYogiParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float l1 = 1;
  void clear_l1();
  static const int kL1FieldNumber = 1;
  float l1() const;
  void set_l1(float value);

  // float l2 = 2;
  void clear_l2();
  static const int kL2FieldNumber = 2;
  float l2() const;
  void set_l2(float value);

  // float beta2 = 3;
  void clear_beta2();
  static const int kBeta2FieldNumber = 3;
  float beta2() const;
  void set_beta2(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.OnlineYogiParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float l1_;
  float l2_;
  float beta2_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ProximalYogiParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.ProximalYogiParameters) */ {
 public:
  ProximalYogiParameters();
  virtual ~ProximalYogiParameters();

  ProximalYogiParameters(const ProximalYogiParameters& from);

  inline ProximalYogiParameters& operator=(const ProximalYogiParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ProximalYogiParameters(ProximalYogiParameters&& from) noexcept
    : ProximalYogiParameters() {
    *this = ::std::move(from);
  }

  inline ProximalYogiParameters& operator=(ProximalYogiParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const ProximalYogiParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ProximalYogiParameters* internal_default_instance() {
    return reinterpret_cast<const ProximalYogiParameters*>(
               &_ProximalYogiParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    17;

  void Swap(ProximalYogiParameters* other);
  friend void swap(ProximalYogiParameters& a, ProximalYogiParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ProximalYogiParameters* New() const final {
    return CreateMaybeMessage<ProximalYogiParameters>(NULL);
  }

  ProximalYogiParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ProximalYogiParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ProximalYogiParameters& from);
  void MergeFrom(const ProximalYogiParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ProximalYogiParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float l1 = 1;
  void clear_l1();
  static const int kL1FieldNumber = 1;
  float l1() const;
  void set_l1(float value);

  // float l2 = 2;
  void clear_l2();
  static const int kL2FieldNumber = 2;
  float l2() const;
  void set_l2(float value);

  // float beta1 = 3;
  void clear_beta1();
  static const int kBeta1FieldNumber = 3;
  float beta1() const;
  void set_beta1(float value);

  // float beta2 = 4;
  void clear_beta2();
  static const int kBeta2FieldNumber = 4;
  float beta2() const;
  void set_beta2(float value);

  // float epsilon = 5;
  void clear_epsilon();
  static const int kEpsilonFieldNumber = 5;
  float epsilon() const;
  void set_epsilon(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.ProximalYogiParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float l1_;
  float l2_;
  float beta1_;
  float beta2_;
  float epsilon_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FrequencyEstimatorParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.FrequencyEstimatorParameters) */ {
 public:
  FrequencyEstimatorParameters();
  virtual ~FrequencyEstimatorParameters();

  FrequencyEstimatorParameters(const FrequencyEstimatorParameters& from);

  inline FrequencyEstimatorParameters& operator=(const FrequencyEstimatorParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  FrequencyEstimatorParameters(FrequencyEstimatorParameters&& from) noexcept
    : FrequencyEstimatorParameters() {
    *this = ::std::move(from);
  }

  inline FrequencyEstimatorParameters& operator=(FrequencyEstimatorParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const FrequencyEstimatorParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const FrequencyEstimatorParameters* internal_default_instance() {
    return reinterpret_cast<const FrequencyEstimatorParameters*>(
               &_FrequencyEstimatorParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    18;

  void Swap(FrequencyEstimatorParameters* other);
  friend void swap(FrequencyEstimatorParameters& a, FrequencyEstimatorParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline FrequencyEstimatorParameters* New() const final {
    return CreateMaybeMessage<FrequencyEstimatorParameters>(NULL);
  }

  FrequencyEstimatorParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<FrequencyEstimatorParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const FrequencyEstimatorParameters& from);
  void MergeFrom(const FrequencyEstimatorParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(FrequencyEstimatorParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // float tau = 1;
  void clear_tau();
  static const int kTauFieldNumber = 1;
  float tau() const;
  void set_tau(float value);

  // float max_delta = 2;
  void clear_max_delta();
  static const int kMaxDeltaFieldNumber = 2;
  float max_delta() const;
  void set_max_delta(float value);

  // float outlier_threshold = 3;
  void clear_outlier_threshold();
  static const int kOutlierThresholdFieldNumber = 3;
  float outlier_threshold() const;
  void set_outlier_threshold(float value);

  // float weight_exponent = 4;
  void clear_weight_exponent();
  static const int kWeightExponentFieldNumber = 4;
  float weight_exponent() const;
  void set_weight_exponent(float value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.FrequencyEstimatorParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  float tau_;
  float max_delta_;
  float outlier_threshold_;
  float weight_exponent_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class UserDefinedProgramParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.UserDefinedProgramParameters) */ {
 public:
  UserDefinedProgramParameters();
  virtual ~UserDefinedProgramParameters();

  UserDefinedProgramParameters(const UserDefinedProgramParameters& from);

  inline UserDefinedProgramParameters& operator=(const UserDefinedProgramParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  UserDefinedProgramParameters(UserDefinedProgramParameters&& from) noexcept
    : UserDefinedProgramParameters() {
    *this = ::std::move(from);
  }

  inline UserDefinedProgramParameters& operator=(UserDefinedProgramParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const UserDefinedProgramParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const UserDefinedProgramParameters* internal_default_instance() {
    return reinterpret_cast<const UserDefinedProgramParameters*>(
               &_UserDefinedProgramParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    19;

  void Swap(UserDefinedProgramParameters* other);
  friend void swap(UserDefinedProgramParameters& a, UserDefinedProgramParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline UserDefinedProgramParameters* New() const final {
    return CreateMaybeMessage<UserDefinedProgramParameters>(NULL);
  }

  UserDefinedProgramParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<UserDefinedProgramParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const UserDefinedProgramParameters& from);
  void MergeFrom(const UserDefinedProgramParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(UserDefinedProgramParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .xla.HloModuleProto program = 1;
  bool has_program() const;
  void clear_program();
  static const int kProgramFieldNumber = 1;
  private:
  const ::xla::HloModuleProto& _internal_program() const;
  public:
  const ::xla::HloModuleProto& program() const;
  ::xla::HloModuleProto* release_program();
  ::xla::HloModuleProto* mutable_program();
  void set_allocated_program(::xla::HloModuleProto* program);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.UserDefinedProgramParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::xla::HloModuleProto* program_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class AssignParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.AssignParameters) */ {
 public:
  AssignParameters();
  virtual ~AssignParameters();

  AssignParameters(const AssignParameters& from);

  inline AssignParameters& operator=(const AssignParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  AssignParameters(AssignParameters&& from) noexcept
    : AssignParameters() {
    *this = ::std::move(from);
  }

  inline AssignParameters& operator=(AssignParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const AssignParameters& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const AssignParameters* internal_default_instance() {
    return reinterpret_cast<const AssignParameters*>(
               &_AssignParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    20;

  void Swap(AssignParameters* other);
  friend void swap(AssignParameters& a, AssignParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline AssignParameters* New() const final {
    return CreateMaybeMessage<AssignParameters>(NULL);
  }

  AssignParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<AssignParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const AssignParameters& from);
  void MergeFrom(const AssignParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(AssignParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.AssignParameters)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class GradientAccumulationStatus : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.GradientAccumulationStatus) */ {
 public:
  GradientAccumulationStatus();
  virtual ~GradientAccumulationStatus();

  GradientAccumulationStatus(const GradientAccumulationStatus& from);

  inline GradientAccumulationStatus& operator=(const GradientAccumulationStatus& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  GradientAccumulationStatus(GradientAccumulationStatus&& from) noexcept
    : GradientAccumulationStatus() {
    *this = ::std::move(from);
  }

  inline GradientAccumulationStatus& operator=(GradientAccumulationStatus&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const GradientAccumulationStatus& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const GradientAccumulationStatus* internal_default_instance() {
    return reinterpret_cast<const GradientAccumulationStatus*>(
               &_GradientAccumulationStatus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    21;

  void Swap(GradientAccumulationStatus* other);
  friend void swap(GradientAccumulationStatus& a, GradientAccumulationStatus& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline GradientAccumulationStatus* New() const final {
    return CreateMaybeMessage<GradientAccumulationStatus>(NULL);
  }

  GradientAccumulationStatus* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<GradientAccumulationStatus>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const GradientAccumulationStatus& from);
  void MergeFrom(const GradientAccumulationStatus& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(GradientAccumulationStatus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef GradientAccumulationStatus_Status Status;
  static const Status UNSPECIFIED =
    GradientAccumulationStatus_Status_UNSPECIFIED;
  static const Status ENABLED =
    GradientAccumulationStatus_Status_ENABLED;
  static const Status DISABLED =
    GradientAccumulationStatus_Status_DISABLED;
  static inline bool Status_IsValid(int value) {
    return GradientAccumulationStatus_Status_IsValid(value);
  }
  static const Status Status_MIN =
    GradientAccumulationStatus_Status_Status_MIN;
  static const Status Status_MAX =
    GradientAccumulationStatus_Status_Status_MAX;
  static const int Status_ARRAYSIZE =
    GradientAccumulationStatus_Status_Status_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Status_descriptor() {
    return GradientAccumulationStatus_Status_descriptor();
  }
  static inline const ::std::string& Status_Name(Status value) {
    return GradientAccumulationStatus_Status_Name(value);
  }
  static inline bool Status_Parse(const ::std::string& name,
      Status* value) {
    return GradientAccumulationStatus_Status_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.GradientAccumulationStatus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class LowDimensionalPackingStatus : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.LowDimensionalPackingStatus) */ {
 public:
  LowDimensionalPackingStatus();
  virtual ~LowDimensionalPackingStatus();

  LowDimensionalPackingStatus(const LowDimensionalPackingStatus& from);

  inline LowDimensionalPackingStatus& operator=(const LowDimensionalPackingStatus& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  LowDimensionalPackingStatus(LowDimensionalPackingStatus&& from) noexcept
    : LowDimensionalPackingStatus() {
    *this = ::std::move(from);
  }

  inline LowDimensionalPackingStatus& operator=(LowDimensionalPackingStatus&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const LowDimensionalPackingStatus& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const LowDimensionalPackingStatus* internal_default_instance() {
    return reinterpret_cast<const LowDimensionalPackingStatus*>(
               &_LowDimensionalPackingStatus_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    22;

  void Swap(LowDimensionalPackingStatus* other);
  friend void swap(LowDimensionalPackingStatus& a, LowDimensionalPackingStatus& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline LowDimensionalPackingStatus* New() const final {
    return CreateMaybeMessage<LowDimensionalPackingStatus>(NULL);
  }

  LowDimensionalPackingStatus* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<LowDimensionalPackingStatus>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const LowDimensionalPackingStatus& from);
  void MergeFrom(const LowDimensionalPackingStatus& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(LowDimensionalPackingStatus* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef LowDimensionalPackingStatus_Status Status;
  static const Status UNSPECIFIED =
    LowDimensionalPackingStatus_Status_UNSPECIFIED;
  static const Status ENABLED =
    LowDimensionalPackingStatus_Status_ENABLED;
  static const Status DISABLED =
    LowDimensionalPackingStatus_Status_DISABLED;
  static inline bool Status_IsValid(int value) {
    return LowDimensionalPackingStatus_Status_IsValid(value);
  }
  static const Status Status_MIN =
    LowDimensionalPackingStatus_Status_Status_MIN;
  static const Status Status_MAX =
    LowDimensionalPackingStatus_Status_Status_MAX;
  static const int Status_ARRAYSIZE =
    LowDimensionalPackingStatus_Status_Status_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Status_descriptor() {
    return LowDimensionalPackingStatus_Status_descriptor();
  }
  static inline const ::std::string& Status_Name(Status value) {
    return LowDimensionalPackingStatus_Status_Name(value);
  }
  static inline bool Status_Parse(const ::std::string& name,
      Status* value) {
    return LowDimensionalPackingStatus_Status_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.LowDimensionalPackingStatus)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class HotIdReplicationConfiguration : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.HotIdReplicationConfiguration) */ {
 public:
  HotIdReplicationConfiguration();
  virtual ~HotIdReplicationConfiguration();

  HotIdReplicationConfiguration(const HotIdReplicationConfiguration& from);

  inline HotIdReplicationConfiguration& operator=(const HotIdReplicationConfiguration& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  HotIdReplicationConfiguration(HotIdReplicationConfiguration&& from) noexcept
    : HotIdReplicationConfiguration() {
    *this = ::std::move(from);
  }

  inline HotIdReplicationConfiguration& operator=(HotIdReplicationConfiguration&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const HotIdReplicationConfiguration& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const HotIdReplicationConfiguration* internal_default_instance() {
    return reinterpret_cast<const HotIdReplicationConfiguration*>(
               &_HotIdReplicationConfiguration_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    23;

  void Swap(HotIdReplicationConfiguration* other);
  friend void swap(HotIdReplicationConfiguration& a, HotIdReplicationConfiguration& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline HotIdReplicationConfiguration* New() const final {
    return CreateMaybeMessage<HotIdReplicationConfiguration>(NULL);
  }

  HotIdReplicationConfiguration* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<HotIdReplicationConfiguration>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const HotIdReplicationConfiguration& from);
  void MergeFrom(const HotIdReplicationConfiguration& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(HotIdReplicationConfiguration* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef HotIdReplicationConfiguration_Status Status;
  static const Status UNSPECIFIED =
    HotIdReplicationConfiguration_Status_UNSPECIFIED;
  static const Status ENABLED =
    HotIdReplicationConfiguration_Status_ENABLED;
  static const Status DISABLED =
    HotIdReplicationConfiguration_Status_DISABLED;
  static inline bool Status_IsValid(int value) {
    return HotIdReplicationConfiguration_Status_IsValid(value);
  }
  static const Status Status_MIN =
    HotIdReplicationConfiguration_Status_Status_MIN;
  static const Status Status_MAX =
    HotIdReplicationConfiguration_Status_Status_MAX;
  static const int Status_ARRAYSIZE =
    HotIdReplicationConfiguration_Status_Status_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Status_descriptor() {
    return HotIdReplicationConfiguration_Status_descriptor();
  }
  static inline const ::std::string& Status_Name(Status value) {
    return HotIdReplicationConfiguration_Status_Name(value);
  }
  static inline bool Status_Parse(const ::std::string& name,
      Status* value) {
    return HotIdReplicationConfiguration_Status_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // .tensorflow.tpu.HotIdReplicationConfiguration.Status status = 1;
  void clear_status();
  static const int kStatusFieldNumber = 1;
  ::tensorflow::tpu::HotIdReplicationConfiguration_Status status() const;
  void set_status(::tensorflow::tpu::HotIdReplicationConfiguration_Status value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.HotIdReplicationConfiguration)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  int status_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class OptimizationParameters : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.OptimizationParameters) */ {
 public:
  OptimizationParameters();
  virtual ~OptimizationParameters();

  OptimizationParameters(const OptimizationParameters& from);

  inline OptimizationParameters& operator=(const OptimizationParameters& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  OptimizationParameters(OptimizationParameters&& from) noexcept
    : OptimizationParameters() {
    *this = ::std::move(from);
  }

  inline OptimizationParameters& operator=(OptimizationParameters&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const OptimizationParameters& default_instance();

  enum ParametersCase {
    kAdagrad = 3,
    kAdagradMomentum = 26,
    kBoundedAdagrad = 19,
    kStochasticGradientDescent = 4,
    kFtrl = 5,
    kAdam = 6,
    kMomentum = 8,
    kRmsProp = 9,
    kCenteredRmsProp = 10,
    kMdlAdagradLight = 11,
    kAdadelta = 12,
    kProximalAdagrad = 14,
    kOnlineYogi = 20,
    kProximalYogi = 21,
    kFrequencyEstimator = 23,
    kUserDefinedProgram = 24,
    kAssign = 25,
    PARAMETERS_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const OptimizationParameters* internal_default_instance() {
    return reinterpret_cast<const OptimizationParameters*>(
               &_OptimizationParameters_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    24;

  void Swap(OptimizationParameters* other);
  friend void swap(OptimizationParameters& a, OptimizationParameters& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline OptimizationParameters* New() const final {
    return CreateMaybeMessage<OptimizationParameters>(NULL);
  }

  OptimizationParameters* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<OptimizationParameters>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const OptimizationParameters& from);
  void MergeFrom(const OptimizationParameters& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(OptimizationParameters* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // .tensorflow.tpu.ClippingLimits clipping_limits = 2;
  bool has_clipping_limits() const;
  void clear_clipping_limits();
  static const int kClippingLimitsFieldNumber = 2;
  private:
  const ::tensorflow::tpu::ClippingLimits& _internal_clipping_limits() const;
  public:
  const ::tensorflow::tpu::ClippingLimits& clipping_limits() const;
  ::tensorflow::tpu::ClippingLimits* release_clipping_limits();
  ::tensorflow::tpu::ClippingLimits* mutable_clipping_limits();
  void set_allocated_clipping_limits(::tensorflow::tpu::ClippingLimits* clipping_limits);

  // .tensorflow.tpu.ClippingLimits gradient_clipping_limits = 7;
  bool has_gradient_clipping_limits() const;
  void clear_gradient_clipping_limits();
  static const int kGradientClippingLimitsFieldNumber = 7;
  private:
  const ::tensorflow::tpu::ClippingLimits& _internal_gradient_clipping_limits() const;
  public:
  const ::tensorflow::tpu::ClippingLimits& gradient_clipping_limits() const;
  ::tensorflow::tpu::ClippingLimits* release_gradient_clipping_limits();
  ::tensorflow::tpu::ClippingLimits* mutable_gradient_clipping_limits();
  void set_allocated_gradient_clipping_limits(::tensorflow::tpu::ClippingLimits* gradient_clipping_limits);

  // .tensorflow.tpu.LearningRate learning_rate = 13;
  bool has_learning_rate() const;
  void clear_learning_rate();
  static const int kLearningRateFieldNumber = 13;
  private:
  const ::tensorflow::tpu::LearningRate& _internal_learning_rate() const;
  public:
  const ::tensorflow::tpu::LearningRate& learning_rate() const;
  ::tensorflow::tpu::LearningRate* release_learning_rate();
  ::tensorflow::tpu::LearningRate* mutable_learning_rate();
  void set_allocated_learning_rate(::tensorflow::tpu::LearningRate* learning_rate);

  // .tensorflow.tpu.HotIdReplicationConfiguration hot_id_replication_configuration = 18;
  bool has_hot_id_replication_configuration() const;
  void clear_hot_id_replication_configuration();
  static const int kHotIdReplicationConfigurationFieldNumber = 18;
  private:
  const ::tensorflow::tpu::HotIdReplicationConfiguration& _internal_hot_id_replication_configuration() const;
  public:
  const ::tensorflow::tpu::HotIdReplicationConfiguration& hot_id_replication_configuration() const;
  ::tensorflow::tpu::HotIdReplicationConfiguration* release_hot_id_replication_configuration();
  ::tensorflow::tpu::HotIdReplicationConfiguration* mutable_hot_id_replication_configuration();
  void set_allocated_hot_id_replication_configuration(::tensorflow::tpu::HotIdReplicationConfiguration* hot_id_replication_configuration);

  // .tensorflow.tpu.SimulatedQuantization simulated_quantization = 27;
  bool has_simulated_quantization() const;
  void clear_simulated_quantization();
  static const int kSimulatedQuantizationFieldNumber = 27;
  private:
  const ::tensorflow::tpu::SimulatedQuantization& _internal_simulated_quantization() const;
  public:
  const ::tensorflow::tpu::SimulatedQuantization& simulated_quantization() const;
  ::tensorflow::tpu::SimulatedQuantization* release_simulated_quantization();
  ::tensorflow::tpu::SimulatedQuantization* mutable_simulated_quantization();
  void set_allocated_simulated_quantization(::tensorflow::tpu::SimulatedQuantization* simulated_quantization);

  // float weight_decay_factor = 16;
  void clear_weight_decay_factor();
  static const int kWeightDecayFactorFieldNumber = 16;
  float weight_decay_factor() const;
  void set_weight_decay_factor(float value);

  // .tensorflow.tpu.GradientAccumulationStatus.Status gradient_accumulation_status = 17;
  void clear_gradient_accumulation_status();
  static const int kGradientAccumulationStatusFieldNumber = 17;
  ::tensorflow::tpu::GradientAccumulationStatus_Status gradient_accumulation_status() const;
  void set_gradient_accumulation_status(::tensorflow::tpu::GradientAccumulationStatus_Status value);

  // bool multiply_weight_decay_factor_by_learning_rate = 22;
  void clear_multiply_weight_decay_factor_by_learning_rate();
  static const int kMultiplyWeightDecayFactorByLearningRateFieldNumber = 22;
  bool multiply_weight_decay_factor_by_learning_rate() const;
  void set_multiply_weight_decay_factor_by_learning_rate(bool value);

  // .tensorflow.tpu.LowDimensionalPackingStatus.Status low_dimensional_packing_status = 28;
  void clear_low_dimensional_packing_status();
  static const int kLowDimensionalPackingStatusFieldNumber = 28;
  ::tensorflow::tpu::LowDimensionalPackingStatus_Status low_dimensional_packing_status() const;
  void set_low_dimensional_packing_status(::tensorflow::tpu::LowDimensionalPackingStatus_Status value);

  // .tensorflow.tpu.AdagradParameters adagrad = 3;
  bool has_adagrad() const;
  void clear_adagrad();
  static const int kAdagradFieldNumber = 3;
  private:
  const ::tensorflow::tpu::AdagradParameters& _internal_adagrad() const;
  public:
  const ::tensorflow::tpu::AdagradParameters& adagrad() const;
  ::tensorflow::tpu::AdagradParameters* release_adagrad();
  ::tensorflow::tpu::AdagradParameters* mutable_adagrad();
  void set_allocated_adagrad(::tensorflow::tpu::AdagradParameters* adagrad);

  // .tensorflow.tpu.AdagradMomentumParameters adagrad_momentum = 26;
  bool has_adagrad_momentum() const;
  void clear_adagrad_momentum();
  static const int kAdagradMomentumFieldNumber = 26;
  private:
  const ::tensorflow::tpu::AdagradMomentumParameters& _internal_adagrad_momentum() const;
  public:
  const ::tensorflow::tpu::AdagradMomentumParameters& adagrad_momentum() const;
  ::tensorflow::tpu::AdagradMomentumParameters* release_adagrad_momentum();
  ::tensorflow::tpu::AdagradMomentumParameters* mutable_adagrad_momentum();
  void set_allocated_adagrad_momentum(::tensorflow::tpu::AdagradMomentumParameters* adagrad_momentum);

  // .tensorflow.tpu.BoundedAdagradParameters bounded_adagrad = 19;
  bool has_bounded_adagrad() const;
  void clear_bounded_adagrad();
  static const int kBoundedAdagradFieldNumber = 19;
  private:
  const ::tensorflow::tpu::BoundedAdagradParameters& _internal_bounded_adagrad() const;
  public:
  const ::tensorflow::tpu::BoundedAdagradParameters& bounded_adagrad() const;
  ::tensorflow::tpu::BoundedAdagradParameters* release_bounded_adagrad();
  ::tensorflow::tpu::BoundedAdagradParameters* mutable_bounded_adagrad();
  void set_allocated_bounded_adagrad(::tensorflow::tpu::BoundedAdagradParameters* bounded_adagrad);

  // .tensorflow.tpu.StochasticGradientDescentParameters stochastic_gradient_descent = 4;
  bool has_stochastic_gradient_descent() const;
  void clear_stochastic_gradient_descent();
  static const int kStochasticGradientDescentFieldNumber = 4;
  private:
  const ::tensorflow::tpu::StochasticGradientDescentParameters& _internal_stochastic_gradient_descent() const;
  public:
  const ::tensorflow::tpu::StochasticGradientDescentParameters& stochastic_gradient_descent() const;
  ::tensorflow::tpu::StochasticGradientDescentParameters* release_stochastic_gradient_descent();
  ::tensorflow::tpu::StochasticGradientDescentParameters* mutable_stochastic_gradient_descent();
  void set_allocated_stochastic_gradient_descent(::tensorflow::tpu::StochasticGradientDescentParameters* stochastic_gradient_descent);

  // .tensorflow.tpu.FtrlParameters ftrl = 5;
  bool has_ftrl() const;
  void clear_ftrl();
  static const int kFtrlFieldNumber = 5;
  private:
  const ::tensorflow::tpu::FtrlParameters& _internal_ftrl() const;
  public:
  const ::tensorflow::tpu::FtrlParameters& ftrl() const;
  ::tensorflow::tpu::FtrlParameters* release_ftrl();
  ::tensorflow::tpu::FtrlParameters* mutable_ftrl();
  void set_allocated_ftrl(::tensorflow::tpu::FtrlParameters* ftrl);

  // .tensorflow.tpu.AdamParameters adam = 6;
  bool has_adam() const;
  void clear_adam();
  static const int kAdamFieldNumber = 6;
  private:
  const ::tensorflow::tpu::AdamParameters& _internal_adam() const;
  public:
  const ::tensorflow::tpu::AdamParameters& adam() const;
  ::tensorflow::tpu::AdamParameters* release_adam();
  ::tensorflow::tpu::AdamParameters* mutable_adam();
  void set_allocated_adam(::tensorflow::tpu::AdamParameters* adam);

  // .tensorflow.tpu.MomentumParameters momentum = 8;
  bool has_momentum() const;
  void clear_momentum();
  static const int kMomentumFieldNumber = 8;
  private:
  const ::tensorflow::tpu::MomentumParameters& _internal_momentum() const;
  public:
  const ::tensorflow::tpu::MomentumParameters& momentum() const;
  ::tensorflow::tpu::MomentumParameters* release_momentum();
  ::tensorflow::tpu::MomentumParameters* mutable_momentum();
  void set_allocated_momentum(::tensorflow::tpu::MomentumParameters* momentum);

  // .tensorflow.tpu.RmsPropParameters rms_prop = 9;
  bool has_rms_prop() const;
  void clear_rms_prop();
  static const int kRmsPropFieldNumber = 9;
  private:
  const ::tensorflow::tpu::RmsPropParameters& _internal_rms_prop() const;
  public:
  const ::tensorflow::tpu::RmsPropParameters& rms_prop() const;
  ::tensorflow::tpu::RmsPropParameters* release_rms_prop();
  ::tensorflow::tpu::RmsPropParameters* mutable_rms_prop();
  void set_allocated_rms_prop(::tensorflow::tpu::RmsPropParameters* rms_prop);

  // .tensorflow.tpu.CenteredRmsPropParameters centered_rms_prop = 10;
  bool has_centered_rms_prop() const;
  void clear_centered_rms_prop();
  static const int kCenteredRmsPropFieldNumber = 10;
  private:
  const ::tensorflow::tpu::CenteredRmsPropParameters& _internal_centered_rms_prop() const;
  public:
  const ::tensorflow::tpu::CenteredRmsPropParameters& centered_rms_prop() const;
  ::tensorflow::tpu::CenteredRmsPropParameters* release_centered_rms_prop();
  ::tensorflow::tpu::CenteredRmsPropParameters* mutable_centered_rms_prop();
  void set_allocated_centered_rms_prop(::tensorflow::tpu::CenteredRmsPropParameters* centered_rms_prop);

  // .tensorflow.tpu.MdlAdagradLightParameters mdl_adagrad_light = 11;
  bool has_mdl_adagrad_light() const;
  void clear_mdl_adagrad_light();
  static const int kMdlAdagradLightFieldNumber = 11;
  private:
  const ::tensorflow::tpu::MdlAdagradLightParameters& _internal_mdl_adagrad_light() const;
  public:
  const ::tensorflow::tpu::MdlAdagradLightParameters& mdl_adagrad_light() const;
  ::tensorflow::tpu::MdlAdagradLightParameters* release_mdl_adagrad_light();
  ::tensorflow::tpu::MdlAdagradLightParameters* mutable_mdl_adagrad_light();
  void set_allocated_mdl_adagrad_light(::tensorflow::tpu::MdlAdagradLightParameters* mdl_adagrad_light);

  // .tensorflow.tpu.AdadeltaParameters adadelta = 12;
  bool has_adadelta() const;
  void clear_adadelta();
  static const int kAdadeltaFieldNumber = 12;
  private:
  const ::tensorflow::tpu::AdadeltaParameters& _internal_adadelta() const;
  public:
  const ::tensorflow::tpu::AdadeltaParameters& adadelta() const;
  ::tensorflow::tpu::AdadeltaParameters* release_adadelta();
  ::tensorflow::tpu::AdadeltaParameters* mutable_adadelta();
  void set_allocated_adadelta(::tensorflow::tpu::AdadeltaParameters* adadelta);

  // .tensorflow.tpu.ProximalAdagradParameters proximal_adagrad = 14;
  bool has_proximal_adagrad() const;
  void clear_proximal_adagrad();
  static const int kProximalAdagradFieldNumber = 14;
  private:
  const ::tensorflow::tpu::ProximalAdagradParameters& _internal_proximal_adagrad() const;
  public:
  const ::tensorflow::tpu::ProximalAdagradParameters& proximal_adagrad() const;
  ::tensorflow::tpu::ProximalAdagradParameters* release_proximal_adagrad();
  ::tensorflow::tpu::ProximalAdagradParameters* mutable_proximal_adagrad();
  void set_allocated_proximal_adagrad(::tensorflow::tpu::ProximalAdagradParameters* proximal_adagrad);

  // .tensorflow.tpu.OnlineYogiParameters online_yogi = 20;
  bool has_online_yogi() const;
  void clear_online_yogi();
  static const int kOnlineYogiFieldNumber = 20;
  private:
  const ::tensorflow::tpu::OnlineYogiParameters& _internal_online_yogi() const;
  public:
  const ::tensorflow::tpu::OnlineYogiParameters& online_yogi() const;
  ::tensorflow::tpu::OnlineYogiParameters* release_online_yogi();
  ::tensorflow::tpu::OnlineYogiParameters* mutable_online_yogi();
  void set_allocated_online_yogi(::tensorflow::tpu::OnlineYogiParameters* online_yogi);

  // .tensorflow.tpu.ProximalYogiParameters proximal_yogi = 21;
  bool has_proximal_yogi() const;
  void clear_proximal_yogi();
  static const int kProximalYogiFieldNumber = 21;
  private:
  const ::tensorflow::tpu::ProximalYogiParameters& _internal_proximal_yogi() const;
  public:
  const ::tensorflow::tpu::ProximalYogiParameters& proximal_yogi() const;
  ::tensorflow::tpu::ProximalYogiParameters* release_proximal_yogi();
  ::tensorflow::tpu::ProximalYogiParameters* mutable_proximal_yogi();
  void set_allocated_proximal_yogi(::tensorflow::tpu::ProximalYogiParameters* proximal_yogi);

  // .tensorflow.tpu.FrequencyEstimatorParameters frequency_estimator = 23;
  bool has_frequency_estimator() const;
  void clear_frequency_estimator();
  static const int kFrequencyEstimatorFieldNumber = 23;
  private:
  const ::tensorflow::tpu::FrequencyEstimatorParameters& _internal_frequency_estimator() const;
  public:
  const ::tensorflow::tpu::FrequencyEstimatorParameters& frequency_estimator() const;
  ::tensorflow::tpu::FrequencyEstimatorParameters* release_frequency_estimator();
  ::tensorflow::tpu::FrequencyEstimatorParameters* mutable_frequency_estimator();
  void set_allocated_frequency_estimator(::tensorflow::tpu::FrequencyEstimatorParameters* frequency_estimator);

  // .tensorflow.tpu.UserDefinedProgramParameters user_defined_program = 24;
  bool has_user_defined_program() const;
  void clear_user_defined_program();
  static const int kUserDefinedProgramFieldNumber = 24;
  private:
  const ::tensorflow::tpu::UserDefinedProgramParameters& _internal_user_defined_program() const;
  public:
  const ::tensorflow::tpu::UserDefinedProgramParameters& user_defined_program() const;
  ::tensorflow::tpu::UserDefinedProgramParameters* release_user_defined_program();
  ::tensorflow::tpu::UserDefinedProgramParameters* mutable_user_defined_program();
  void set_allocated_user_defined_program(::tensorflow::tpu::UserDefinedProgramParameters* user_defined_program);

  // .tensorflow.tpu.AssignParameters assign = 25;
  bool has_assign() const;
  void clear_assign();
  static const int kAssignFieldNumber = 25;
  private:
  const ::tensorflow::tpu::AssignParameters& _internal_assign() const;
  public:
  const ::tensorflow::tpu::AssignParameters& assign() const;
  ::tensorflow::tpu::AssignParameters* release_assign();
  ::tensorflow::tpu::AssignParameters* mutable_assign();
  void set_allocated_assign(::tensorflow::tpu::AssignParameters* assign);

  void clear_parameters();
  ParametersCase parameters_case() const;
  // @@protoc_insertion_point(class_scope:tensorflow.tpu.OptimizationParameters)
 private:
  void set_has_adagrad();
  void set_has_adagrad_momentum();
  void set_has_bounded_adagrad();
  void set_has_stochastic_gradient_descent();
  void set_has_ftrl();
  void set_has_adam();
  void set_has_momentum();
  void set_has_rms_prop();
  void set_has_centered_rms_prop();
  void set_has_mdl_adagrad_light();
  void set_has_adadelta();
  void set_has_proximal_adagrad();
  void set_has_online_yogi();
  void set_has_proximal_yogi();
  void set_has_frequency_estimator();
  void set_has_user_defined_program();
  void set_has_assign();

  inline bool has_parameters() const;
  inline void clear_has_parameters();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::tensorflow::tpu::ClippingLimits* clipping_limits_;
  ::tensorflow::tpu::ClippingLimits* gradient_clipping_limits_;
  ::tensorflow::tpu::LearningRate* learning_rate_;
  ::tensorflow::tpu::HotIdReplicationConfiguration* hot_id_replication_configuration_;
  ::tensorflow::tpu::SimulatedQuantization* simulated_quantization_;
  float weight_decay_factor_;
  int gradient_accumulation_status_;
  bool multiply_weight_decay_factor_by_learning_rate_;
  int low_dimensional_packing_status_;
  union ParametersUnion {
    ParametersUnion() {}
    ::tensorflow::tpu::AdagradParameters* adagrad_;
    ::tensorflow::tpu::AdagradMomentumParameters* adagrad_momentum_;
    ::tensorflow::tpu::BoundedAdagradParameters* bounded_adagrad_;
    ::tensorflow::tpu::StochasticGradientDescentParameters* stochastic_gradient_descent_;
    ::tensorflow::tpu::FtrlParameters* ftrl_;
    ::tensorflow::tpu::AdamParameters* adam_;
    ::tensorflow::tpu::MomentumParameters* momentum_;
    ::tensorflow::tpu::RmsPropParameters* rms_prop_;
    ::tensorflow::tpu::CenteredRmsPropParameters* centered_rms_prop_;
    ::tensorflow::tpu::MdlAdagradLightParameters* mdl_adagrad_light_;
    ::tensorflow::tpu::AdadeltaParameters* adadelta_;
    ::tensorflow::tpu::ProximalAdagradParameters* proximal_adagrad_;
    ::tensorflow::tpu::OnlineYogiParameters* online_yogi_;
    ::tensorflow::tpu::ProximalYogiParameters* proximal_yogi_;
    ::tensorflow::tpu::FrequencyEstimatorParameters* frequency_estimator_;
    ::tensorflow::tpu::UserDefinedProgramParameters* user_defined_program_;
    ::tensorflow::tpu::AssignParameters* assign_;
  } parameters_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StateVariableSpecification_UserDefined : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.StateVariableSpecification.UserDefined) */ {
 public:
  StateVariableSpecification_UserDefined();
  virtual ~StateVariableSpecification_UserDefined();

  StateVariableSpecification_UserDefined(const StateVariableSpecification_UserDefined& from);

  inline StateVariableSpecification_UserDefined& operator=(const StateVariableSpecification_UserDefined& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StateVariableSpecification_UserDefined(StateVariableSpecification_UserDefined&& from) noexcept
    : StateVariableSpecification_UserDefined() {
    *this = ::std::move(from);
  }

  inline StateVariableSpecification_UserDefined& operator=(StateVariableSpecification_UserDefined&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const StateVariableSpecification_UserDefined& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StateVariableSpecification_UserDefined* internal_default_instance() {
    return reinterpret_cast<const StateVariableSpecification_UserDefined*>(
               &_StateVariableSpecification_UserDefined_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    25;

  void Swap(StateVariableSpecification_UserDefined* other);
  friend void swap(StateVariableSpecification_UserDefined& a, StateVariableSpecification_UserDefined& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StateVariableSpecification_UserDefined* New() const final {
    return CreateMaybeMessage<StateVariableSpecification_UserDefined>(NULL);
  }

  StateVariableSpecification_UserDefined* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StateVariableSpecification_UserDefined>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StateVariableSpecification_UserDefined& from);
  void MergeFrom(const StateVariableSpecification_UserDefined& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StateVariableSpecification_UserDefined* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.StateVariableSpecification.UserDefined)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StateVariableSpecification_FillWithConstant : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.StateVariableSpecification.FillWithConstant) */ {
 public:
  StateVariableSpecification_FillWithConstant();
  virtual ~StateVariableSpecification_FillWithConstant();

  StateVariableSpecification_FillWithConstant(const StateVariableSpecification_FillWithConstant& from);

  inline StateVariableSpecification_FillWithConstant& operator=(const StateVariableSpecification_FillWithConstant& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StateVariableSpecification_FillWithConstant(StateVariableSpecification_FillWithConstant&& from) noexcept
    : StateVariableSpecification_FillWithConstant() {
    *this = ::std::move(from);
  }

  inline StateVariableSpecification_FillWithConstant& operator=(StateVariableSpecification_FillWithConstant&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const StateVariableSpecification_FillWithConstant& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StateVariableSpecification_FillWithConstant* internal_default_instance() {
    return reinterpret_cast<const StateVariableSpecification_FillWithConstant*>(
               &_StateVariableSpecification_FillWithConstant_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    26;

  void Swap(StateVariableSpecification_FillWithConstant* other);
  friend void swap(StateVariableSpecification_FillWithConstant& a, StateVariableSpecification_FillWithConstant& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StateVariableSpecification_FillWithConstant* New() const final {
    return CreateMaybeMessage<StateVariableSpecification_FillWithConstant>(NULL);
  }

  StateVariableSpecification_FillWithConstant* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StateVariableSpecification_FillWithConstant>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StateVariableSpecification_FillWithConstant& from);
  void MergeFrom(const StateVariableSpecification_FillWithConstant& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StateVariableSpecification_FillWithConstant* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // double initial_value = 1;
  void clear_initial_value();
  static const int kInitialValueFieldNumber = 1;
  double initial_value() const;
  void set_initial_value(double value);

  // @@protoc_insertion_point(class_scope:tensorflow.tpu.StateVariableSpecification.FillWithConstant)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  double initial_value_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StateVariableSpecification : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.tpu.StateVariableSpecification) */ {
 public:
  StateVariableSpecification();
  virtual ~StateVariableSpecification();

  StateVariableSpecification(const StateVariableSpecification& from);

  inline StateVariableSpecification& operator=(const StateVariableSpecification& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StateVariableSpecification(StateVariableSpecification&& from) noexcept
    : StateVariableSpecification() {
    *this = ::std::move(from);
  }

  inline StateVariableSpecification& operator=(StateVariableSpecification&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  static const ::google::protobuf::Descriptor* descriptor();
  static const StateVariableSpecification& default_instance();

  enum UsageCase {
    kUserDefined = 2,
    kFillWithConstant = 3,
    USAGE_NOT_SET = 0,
  };

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StateVariableSpecification* internal_default_instance() {
    return reinterpret_cast<const StateVariableSpecification*>(
               &_StateVariableSpecification_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    27;

  void Swap(StateVariableSpecification* other);
  friend void swap(StateVariableSpecification& a, StateVariableSpecification& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StateVariableSpecification* New() const final {
    return CreateMaybeMessage<StateVariableSpecification>(NULL);
  }

  StateVariableSpecification* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StateVariableSpecification>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StateVariableSpecification& from);
  void MergeFrom(const StateVariableSpecification& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StateVariableSpecification* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef StateVariableSpecification_UserDefined UserDefined;
  typedef StateVariableSpecification_FillWithConstant FillWithConstant;

  // accessors -------------------------------------------------------

  // string name = 1;
  void clear_name();
  static const int kNameFieldNumber = 1;
  const ::std::string& name() const;
  void set_name(const ::std::string& value);
  #if LANG_CXX11
  void set_name(::std::string&& value);
  #endif
  void set_name(const char* value);
  void set_name(const char* value, size_t size);
  ::std::string* mutable_name();
  ::std::string* release_name();
  void set_allocated_name(::std::string* name);

  // .tensorflow.tpu.StateVariableSpecification.UserDefined user_defined = 2;
  bool has_user_defined() const;
  void clear_user_defined();
  static const int kUserDefinedFieldNumber = 2;
  private:
  const ::tensorflow::tpu::StateVariableSpecification_UserDefined& _internal_user_defined() const;
  public:
  const ::tensorflow::tpu::StateVariableSpecification_UserDefined& user_defined() const;
  ::tensorflow::tpu::StateVariableSpecification_UserDefined* release_user_defined();
  ::tensorflow::tpu::StateVariableSpecification_UserDefined* mutable_user_defined();
  void set_allocated_user_defined(::tensorflow::tpu::StateVariableSpecification_UserDefined* user_defined);

  // .tensorflow.tpu.StateVariableSpecification.FillWithConstant fill_with_constant = 3;
  bool has_fill_with_constant() const;
  void clear_fill_with_constant();
  static const int kFillWithConstantFieldNumber = 3;
  private:
  const ::tensorflow::tpu::StateVariableSpecification_FillWithConstant& _internal_fill_with_constant() const;
  public:
  const ::tensorflow::tpu::StateVariableSpecification_FillWithConstant& fill_with_constant() const;
  ::tensorflow::tpu::StateVariableSpecification_FillWithConstant* release_fill_with_constant();
  ::tensorflow::tpu::StateVariableSpecification_FillWithConstant* mutable_fill_with_constant();
  void set_allocated_fill_with_constant(::tensorflow::tpu::StateVariableSpecification_FillWithConstant* fill_with_constant);

  void clear_usage();
  UsageCase usage_case() const;
  // @@protoc_insertion_point(class_scope:tensorflow.tpu.StateVariableSpecification)
 private:
  void set_has_user_defined();
  void set_has_fill_with_constant();

  inline bool has_usage() const;
  inline void clear_has_usage();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::ArenaStringPtr name_;
  union UsageUnion {
    UsageUnion() {}
    ::tensorflow::tpu::StateVariableSpecification_UserDefined* user_defined_;
    ::tensorflow::tpu::StateVariableSpecification_FillWithConstant* fill_with_constant_;
  } usage_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend struct ::protobuf_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// ClippingLimits

// .google.protobuf.FloatValue lower = 1;
inline bool ClippingLimits::has_lower() const {
  return this != internal_default_instance() && lower_ != NULL;
}
inline const ::google::protobuf::FloatValue& ClippingLimits::_internal_lower() const {
  return *lower_;
}
inline const ::google::protobuf::FloatValue& ClippingLimits::lower() const {
  const ::google::protobuf::FloatValue* p = lower_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ClippingLimits.lower)
  return p != NULL ? *p : *reinterpret_cast<const ::google::protobuf::FloatValue*>(
      &::google::protobuf::_FloatValue_default_instance_);
}
inline ::google::protobuf::FloatValue* ClippingLimits::release_lower() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.ClippingLimits.lower)
  
  ::google::protobuf::FloatValue* temp = lower_;
  lower_ = NULL;
  return temp;
}
inline ::google::protobuf::FloatValue* ClippingLimits::mutable_lower() {
  
  if (lower_ == NULL) {
    auto* p = CreateMaybeMessage<::google::protobuf::FloatValue>(GetArenaNoVirtual());
    lower_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.ClippingLimits.lower)
  return lower_;
}
inline void ClippingLimits::set_allocated_lower(::google::protobuf::FloatValue* lower) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(lower_);
  }
  if (lower) {
    ::google::protobuf::Arena* submessage_arena =
      reinterpret_cast<::google::protobuf::MessageLite*>(lower)->GetArena();
    if (message_arena != submessage_arena) {
      lower = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, lower, submessage_arena);
    }
    
  } else {
    
  }
  lower_ = lower;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.ClippingLimits.lower)
}

// .google.protobuf.FloatValue upper = 2;
inline bool ClippingLimits::has_upper() const {
  return this != internal_default_instance() && upper_ != NULL;
}
inline const ::google::protobuf::FloatValue& ClippingLimits::_internal_upper() const {
  return *upper_;
}
inline const ::google::protobuf::FloatValue& ClippingLimits::upper() const {
  const ::google::protobuf::FloatValue* p = upper_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ClippingLimits.upper)
  return p != NULL ? *p : *reinterpret_cast<const ::google::protobuf::FloatValue*>(
      &::google::protobuf::_FloatValue_default_instance_);
}
inline ::google::protobuf::FloatValue* ClippingLimits::release_upper() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.ClippingLimits.upper)
  
  ::google::protobuf::FloatValue* temp = upper_;
  upper_ = NULL;
  return temp;
}
inline ::google::protobuf::FloatValue* ClippingLimits::mutable_upper() {
  
  if (upper_ == NULL) {
    auto* p = CreateMaybeMessage<::google::protobuf::FloatValue>(GetArenaNoVirtual());
    upper_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.ClippingLimits.upper)
  return upper_;
}
inline void ClippingLimits::set_allocated_upper(::google::protobuf::FloatValue* upper) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(upper_);
  }
  if (upper) {
    ::google::protobuf::Arena* submessage_arena =
      reinterpret_cast<::google::protobuf::MessageLite*>(upper)->GetArena();
    if (message_arena != submessage_arena) {
      upper = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, upper, submessage_arena);
    }
    
  } else {
    
  }
  upper_ = upper;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.ClippingLimits.upper)
}

// -------------------------------------------------------------------

// SimulatedQuantization

// bool enabled = 1;
inline void SimulatedQuantization::clear_enabled() {
  enabled_ = false;
}
inline bool SimulatedQuantization::enabled() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.SimulatedQuantization.enabled)
  return enabled_;
}
inline void SimulatedQuantization::set_enabled(bool value) {
  
  enabled_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.SimulatedQuantization.enabled)
}

// .tensorflow.tpu.ClippingLimits clipping_limits = 2;
inline bool SimulatedQuantization::has_clipping_limits() const {
  return this != internal_default_instance() && clipping_limits_ != NULL;
}
inline void SimulatedQuantization::clear_clipping_limits() {
  if (GetArenaNoVirtual() == NULL && clipping_limits_ != NULL) {
    delete clipping_limits_;
  }
  clipping_limits_ = NULL;
}
inline const ::tensorflow::tpu::ClippingLimits& SimulatedQuantization::_internal_clipping_limits() const {
  return *clipping_limits_;
}
inline const ::tensorflow::tpu::ClippingLimits& SimulatedQuantization::clipping_limits() const {
  const ::tensorflow::tpu::ClippingLimits* p = clipping_limits_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.SimulatedQuantization.clipping_limits)
  return p != NULL ? *p : *reinterpret_cast<const ::tensorflow::tpu::ClippingLimits*>(
      &::tensorflow::tpu::_ClippingLimits_default_instance_);
}
inline ::tensorflow::tpu::ClippingLimits* SimulatedQuantization::release_clipping_limits() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.SimulatedQuantization.clipping_limits)
  
  ::tensorflow::tpu::ClippingLimits* temp = clipping_limits_;
  clipping_limits_ = NULL;
  return temp;
}
inline ::tensorflow::tpu::ClippingLimits* SimulatedQuantization::mutable_clipping_limits() {
  
  if (clipping_limits_ == NULL) {
    auto* p = CreateMaybeMessage<::tensorflow::tpu::ClippingLimits>(GetArenaNoVirtual());
    clipping_limits_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.SimulatedQuantization.clipping_limits)
  return clipping_limits_;
}
inline void SimulatedQuantization::set_allocated_clipping_limits(::tensorflow::tpu::ClippingLimits* clipping_limits) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete clipping_limits_;
  }
  if (clipping_limits) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      clipping_limits = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, clipping_limits, submessage_arena);
    }
    
  } else {
    
  }
  clipping_limits_ = clipping_limits;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.SimulatedQuantization.clipping_limits)
}

// int32 num_buckets = 3;
inline void SimulatedQuantization::clear_num_buckets() {
  num_buckets_ = 0;
}
inline ::google::protobuf::int32 SimulatedQuantization::num_buckets() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.SimulatedQuantization.num_buckets)
  return num_buckets_;
}
inline void SimulatedQuantization::set_num_buckets(::google::protobuf::int32 value) {
  
  num_buckets_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.SimulatedQuantization.num_buckets)
}

// -------------------------------------------------------------------

// DynamicLearningRate

// int32 tag = 1;
inline void DynamicLearningRate::clear_tag() {
  tag_ = 0;
}
inline ::google::protobuf::int32 DynamicLearningRate::tag() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.DynamicLearningRate.tag)
  return tag_;
}
inline void DynamicLearningRate::set_tag(::google::protobuf::int32 value) {
  
  tag_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.DynamicLearningRate.tag)
}

// -------------------------------------------------------------------

// LearningRate

// float constant = 1;
inline bool LearningRate::has_constant() const {
  return learning_rate_case() == kConstant;
}
inline void LearningRate::set_has_constant() {
  _oneof_case_[0] = kConstant;
}
inline void LearningRate::clear_constant() {
  if (has_constant()) {
    learning_rate_.constant_ = 0;
    clear_has_learning_rate();
  }
}
inline float LearningRate::constant() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.LearningRate.constant)
  if (has_constant()) {
    return learning_rate_.constant_;
  }
  return 0;
}
inline void LearningRate::set_constant(float value) {
  if (!has_constant()) {
    clear_learning_rate();
    set_has_constant();
  }
  learning_rate_.constant_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.LearningRate.constant)
}

// .tensorflow.tpu.DynamicLearningRate dynamic = 2;
inline bool LearningRate::has_dynamic() const {
  return learning_rate_case() == kDynamic;
}
inline void LearningRate::set_has_dynamic() {
  _oneof_case_[0] = kDynamic;
}
inline void LearningRate::clear_dynamic() {
  if (has_dynamic()) {
    delete learning_rate_.dynamic_;
    clear_has_learning_rate();
  }
}
inline const ::tensorflow::tpu::DynamicLearningRate& LearningRate::_internal_dynamic() const {
  return *learning_rate_.dynamic_;
}
inline ::tensorflow::tpu::DynamicLearningRate* LearningRate::release_dynamic() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.LearningRate.dynamic)
  if (has_dynamic()) {
    clear_has_learning_rate();
      ::tensorflow::tpu::DynamicLearningRate* temp = learning_rate_.dynamic_;
    learning_rate_.dynamic_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::DynamicLearningRate& LearningRate::dynamic() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.LearningRate.dynamic)
  return has_dynamic()
      ? *learning_rate_.dynamic_
      : *reinterpret_cast< ::tensorflow::tpu::DynamicLearningRate*>(&::tensorflow::tpu::_DynamicLearningRate_default_instance_);
}
inline ::tensorflow::tpu::DynamicLearningRate* LearningRate::mutable_dynamic() {
  if (!has_dynamic()) {
    clear_learning_rate();
    set_has_dynamic();
    learning_rate_.dynamic_ = CreateMaybeMessage< ::tensorflow::tpu::DynamicLearningRate >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.LearningRate.dynamic)
  return learning_rate_.dynamic_;
}

inline bool LearningRate::has_learning_rate() const {
  return learning_rate_case() != LEARNING_RATE_NOT_SET;
}
inline void LearningRate::clear_has_learning_rate() {
  _oneof_case_[0] = LEARNING_RATE_NOT_SET;
}
inline LearningRate::LearningRateCase LearningRate::learning_rate_case() const {
  return LearningRate::LearningRateCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// AdagradParameters

// -------------------------------------------------------------------

// AdagradMomentumParameters

// float momentum = 1;
inline void AdagradMomentumParameters::clear_momentum() {
  momentum_ = 0;
}
inline float AdagradMomentumParameters::momentum() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdagradMomentumParameters.momentum)
  return momentum_;
}
inline void AdagradMomentumParameters::set_momentum(float value) {
  
  momentum_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdagradMomentumParameters.momentum)
}

// bool use_nesterov = 2;
inline void AdagradMomentumParameters::clear_use_nesterov() {
  use_nesterov_ = false;
}
inline bool AdagradMomentumParameters::use_nesterov() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdagradMomentumParameters.use_nesterov)
  return use_nesterov_;
}
inline void AdagradMomentumParameters::set_use_nesterov(bool value) {
  
  use_nesterov_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdagradMomentumParameters.use_nesterov)
}

// float exponent = 3;
inline void AdagradMomentumParameters::clear_exponent() {
  exponent_ = 0;
}
inline float AdagradMomentumParameters::exponent() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdagradMomentumParameters.exponent)
  return exponent_;
}
inline void AdagradMomentumParameters::set_exponent(float value) {
  
  exponent_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdagradMomentumParameters.exponent)
}

// float beta2 = 4;
inline void AdagradMomentumParameters::clear_beta2() {
  beta2_ = 0;
}
inline float AdagradMomentumParameters::beta2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdagradMomentumParameters.beta2)
  return beta2_;
}
inline void AdagradMomentumParameters::set_beta2(float value) {
  
  beta2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdagradMomentumParameters.beta2)
}

// float epsilon = 5;
inline void AdagradMomentumParameters::clear_epsilon() {
  epsilon_ = 0;
}
inline float AdagradMomentumParameters::epsilon() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdagradMomentumParameters.epsilon)
  return epsilon_;
}
inline void AdagradMomentumParameters::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdagradMomentumParameters.epsilon)
}

// -------------------------------------------------------------------

// BoundedAdagradParameters

// bool update_accumulator_first = 1;
inline void BoundedAdagradParameters::clear_update_accumulator_first() {
  update_accumulator_first_ = false;
}
inline bool BoundedAdagradParameters::update_accumulator_first() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.BoundedAdagradParameters.update_accumulator_first)
  return update_accumulator_first_;
}
inline void BoundedAdagradParameters::set_update_accumulator_first(bool value) {
  
  update_accumulator_first_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.BoundedAdagradParameters.update_accumulator_first)
}

// float max_var_update = 2;
inline void BoundedAdagradParameters::clear_max_var_update() {
  max_var_update_ = 0;
}
inline float BoundedAdagradParameters::max_var_update() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.BoundedAdagradParameters.max_var_update)
  return max_var_update_;
}
inline void BoundedAdagradParameters::set_max_var_update(float value) {
  
  max_var_update_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.BoundedAdagradParameters.max_var_update)
}

// float max_accumulator = 3;
inline void BoundedAdagradParameters::clear_max_accumulator() {
  max_accumulator_ = 0;
}
inline float BoundedAdagradParameters::max_accumulator() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.BoundedAdagradParameters.max_accumulator)
  return max_accumulator_;
}
inline void BoundedAdagradParameters::set_max_accumulator(float value) {
  
  max_accumulator_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.BoundedAdagradParameters.max_accumulator)
}

// -------------------------------------------------------------------

// StochasticGradientDescentParameters

// -------------------------------------------------------------------

// FtrlParameters

// float l1 = 1;
inline void FtrlParameters::clear_l1() {
  l1_ = 0;
}
inline float FtrlParameters::l1() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FtrlParameters.l1)
  return l1_;
}
inline void FtrlParameters::set_l1(float value) {
  
  l1_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FtrlParameters.l1)
}

// float l2 = 2;
inline void FtrlParameters::clear_l2() {
  l2_ = 0;
}
inline float FtrlParameters::l2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FtrlParameters.l2)
  return l2_;
}
inline void FtrlParameters::set_l2(float value) {
  
  l2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FtrlParameters.l2)
}

// float lr_power = 3;
inline void FtrlParameters::clear_lr_power() {
  lr_power_ = 0;
}
inline float FtrlParameters::lr_power() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FtrlParameters.lr_power)
  return lr_power_;
}
inline void FtrlParameters::set_lr_power(float value) {
  
  lr_power_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FtrlParameters.lr_power)
}

// float beta = 7;
inline void FtrlParameters::clear_beta() {
  beta_ = 0;
}
inline float FtrlParameters::beta() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FtrlParameters.beta)
  return beta_;
}
inline void FtrlParameters::set_beta(float value) {
  
  beta_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FtrlParameters.beta)
}

// bool multiply_linear_by_lr = 6;
inline void FtrlParameters::clear_multiply_linear_by_lr() {
  multiply_linear_by_lr_ = false;
}
inline bool FtrlParameters::multiply_linear_by_lr() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FtrlParameters.multiply_linear_by_lr)
  return multiply_linear_by_lr_;
}
inline void FtrlParameters::set_multiply_linear_by_lr(bool value) {
  
  multiply_linear_by_lr_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FtrlParameters.multiply_linear_by_lr)
}

// bool allow_zero_accumulator = 8 [deprecated = true];
inline void FtrlParameters::clear_allow_zero_accumulator() {
  allow_zero_accumulator_ = false;
}
inline bool FtrlParameters::allow_zero_accumulator() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FtrlParameters.allow_zero_accumulator)
  return allow_zero_accumulator_;
}
inline void FtrlParameters::set_allow_zero_accumulator(bool value) {
  
  allow_zero_accumulator_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FtrlParameters.allow_zero_accumulator)
}

// -------------------------------------------------------------------

// AdamParameters

// float beta1 = 3;
inline void AdamParameters::clear_beta1() {
  beta1_ = 0;
}
inline float AdamParameters::beta1() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdamParameters.beta1)
  return beta1_;
}
inline void AdamParameters::set_beta1(float value) {
  
  beta1_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdamParameters.beta1)
}

// float beta2 = 4;
inline void AdamParameters::clear_beta2() {
  beta2_ = 0;
}
inline float AdamParameters::beta2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdamParameters.beta2)
  return beta2_;
}
inline void AdamParameters::set_beta2(float value) {
  
  beta2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdamParameters.beta2)
}

// float epsilon = 5;
inline void AdamParameters::clear_epsilon() {
  epsilon_ = 0;
}
inline float AdamParameters::epsilon() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdamParameters.epsilon)
  return epsilon_;
}
inline void AdamParameters::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdamParameters.epsilon)
}

// bool use_non_lazy_adam = 8;
inline void AdamParameters::clear_use_non_lazy_adam() {
  use_non_lazy_adam_ = false;
}
inline bool AdamParameters::use_non_lazy_adam() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdamParameters.use_non_lazy_adam)
  return use_non_lazy_adam_;
}
inline void AdamParameters::set_use_non_lazy_adam(bool value) {
  
  use_non_lazy_adam_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdamParameters.use_non_lazy_adam)
}

// bool use_sum_inside_sqrt = 10;
inline void AdamParameters::clear_use_sum_inside_sqrt() {
  use_sum_inside_sqrt_ = false;
}
inline bool AdamParameters::use_sum_inside_sqrt() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdamParameters.use_sum_inside_sqrt)
  return use_sum_inside_sqrt_;
}
inline void AdamParameters::set_use_sum_inside_sqrt(bool value) {
  
  use_sum_inside_sqrt_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdamParameters.use_sum_inside_sqrt)
}

// -------------------------------------------------------------------

// MomentumParameters

// float momentum = 1;
inline void MomentumParameters::clear_momentum() {
  momentum_ = 0;
}
inline float MomentumParameters::momentum() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MomentumParameters.momentum)
  return momentum_;
}
inline void MomentumParameters::set_momentum(float value) {
  
  momentum_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MomentumParameters.momentum)
}

// bool use_nesterov = 2;
inline void MomentumParameters::clear_use_nesterov() {
  use_nesterov_ = false;
}
inline bool MomentumParameters::use_nesterov() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MomentumParameters.use_nesterov)
  return use_nesterov_;
}
inline void MomentumParameters::set_use_nesterov(bool value) {
  
  use_nesterov_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MomentumParameters.use_nesterov)
}

// -------------------------------------------------------------------

// RmsPropParameters

// float rho = 1;
inline void RmsPropParameters::clear_rho() {
  rho_ = 0;
}
inline float RmsPropParameters::rho() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.RmsPropParameters.rho)
  return rho_;
}
inline void RmsPropParameters::set_rho(float value) {
  
  rho_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.RmsPropParameters.rho)
}

// float momentum = 2;
inline void RmsPropParameters::clear_momentum() {
  momentum_ = 0;
}
inline float RmsPropParameters::momentum() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.RmsPropParameters.momentum)
  return momentum_;
}
inline void RmsPropParameters::set_momentum(float value) {
  
  momentum_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.RmsPropParameters.momentum)
}

// float epsilon = 3;
inline void RmsPropParameters::clear_epsilon() {
  epsilon_ = 0;
}
inline float RmsPropParameters::epsilon() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.RmsPropParameters.epsilon)
  return epsilon_;
}
inline void RmsPropParameters::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.RmsPropParameters.epsilon)
}

// -------------------------------------------------------------------

// CenteredRmsPropParameters

// float rho = 1;
inline void CenteredRmsPropParameters::clear_rho() {
  rho_ = 0;
}
inline float CenteredRmsPropParameters::rho() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.CenteredRmsPropParameters.rho)
  return rho_;
}
inline void CenteredRmsPropParameters::set_rho(float value) {
  
  rho_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.CenteredRmsPropParameters.rho)
}

// float momentum = 2;
inline void CenteredRmsPropParameters::clear_momentum() {
  momentum_ = 0;
}
inline float CenteredRmsPropParameters::momentum() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.CenteredRmsPropParameters.momentum)
  return momentum_;
}
inline void CenteredRmsPropParameters::set_momentum(float value) {
  
  momentum_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.CenteredRmsPropParameters.momentum)
}

// float epsilon = 3;
inline void CenteredRmsPropParameters::clear_epsilon() {
  epsilon_ = 0;
}
inline float CenteredRmsPropParameters::epsilon() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.CenteredRmsPropParameters.epsilon)
  return epsilon_;
}
inline void CenteredRmsPropParameters::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.CenteredRmsPropParameters.epsilon)
}

// -------------------------------------------------------------------

// MdlAdagradLightParameters

// float l2 = 1;
inline void MdlAdagradLightParameters::clear_l2() {
  l2_ = 0;
}
inline float MdlAdagradLightParameters::l2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.l2)
  return l2_;
}
inline void MdlAdagradLightParameters::set_l2(float value) {
  
  l2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.l2)
}

// float lr_power = 2;
inline void MdlAdagradLightParameters::clear_lr_power() {
  lr_power_ = 0;
}
inline float MdlAdagradLightParameters::lr_power() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.lr_power)
  return lr_power_;
}
inline void MdlAdagradLightParameters::set_lr_power(float value) {
  
  lr_power_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.lr_power)
}

// float min_servable_mdl_benefit = 3;
inline void MdlAdagradLightParameters::clear_min_servable_mdl_benefit() {
  min_servable_mdl_benefit_ = 0;
}
inline float MdlAdagradLightParameters::min_servable_mdl_benefit() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.min_servable_mdl_benefit)
  return min_servable_mdl_benefit_;
}
inline void MdlAdagradLightParameters::set_min_servable_mdl_benefit(float value) {
  
  min_servable_mdl_benefit_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.min_servable_mdl_benefit)
}

// float mdl_mix_in_margin = 4;
inline void MdlAdagradLightParameters::clear_mdl_mix_in_margin() {
  mdl_mix_in_margin_ = 0;
}
inline float MdlAdagradLightParameters::mdl_mix_in_margin() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.mdl_mix_in_margin)
  return mdl_mix_in_margin_;
}
inline void MdlAdagradLightParameters::set_mdl_mix_in_margin(float value) {
  
  mdl_mix_in_margin_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.mdl_mix_in_margin)
}

// float mdl_benefit_rampup_coeff = 5;
inline void MdlAdagradLightParameters::clear_mdl_benefit_rampup_coeff() {
  mdl_benefit_rampup_coeff_ = 0;
}
inline float MdlAdagradLightParameters::mdl_benefit_rampup_coeff() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.mdl_benefit_rampup_coeff)
  return mdl_benefit_rampup_coeff_;
}
inline void MdlAdagradLightParameters::set_mdl_benefit_rampup_coeff(float value) {
  
  mdl_benefit_rampup_coeff_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.mdl_benefit_rampup_coeff)
}

// float mdl_min_weight = 6;
inline void MdlAdagradLightParameters::clear_mdl_min_weight() {
  mdl_min_weight_ = 0;
}
inline float MdlAdagradLightParameters::mdl_min_weight() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.mdl_min_weight)
  return mdl_min_weight_;
}
inline void MdlAdagradLightParameters::set_mdl_min_weight(float value) {
  
  mdl_min_weight_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.mdl_min_weight)
}

// float benefit_revisit_scale = 7;
inline void MdlAdagradLightParameters::clear_benefit_revisit_scale() {
  benefit_revisit_scale_ = 0;
}
inline float MdlAdagradLightParameters::benefit_revisit_scale() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.benefit_revisit_scale)
  return benefit_revisit_scale_;
}
inline void MdlAdagradLightParameters::set_benefit_revisit_scale(float value) {
  
  benefit_revisit_scale_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.benefit_revisit_scale)
}

// float max_event_benefit = 8;
inline void MdlAdagradLightParameters::clear_max_event_benefit() {
  max_event_benefit_ = 0;
}
inline float MdlAdagradLightParameters::max_event_benefit() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.max_event_benefit)
  return max_event_benefit_;
}
inline void MdlAdagradLightParameters::set_max_event_benefit(float value) {
  
  max_event_benefit_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.max_event_benefit)
}

// float max_total_benefit = 9;
inline void MdlAdagradLightParameters::clear_max_total_benefit() {
  max_total_benefit_ = 0;
}
inline float MdlAdagradLightParameters::max_total_benefit() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.max_total_benefit)
  return max_total_benefit_;
}
inline void MdlAdagradLightParameters::set_max_total_benefit(float value) {
  
  max_total_benefit_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.max_total_benefit)
}

// float mdl_hard_limit = 10;
inline void MdlAdagradLightParameters::clear_mdl_hard_limit() {
  mdl_hard_limit_ = 0;
}
inline float MdlAdagradLightParameters::mdl_hard_limit() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.mdl_hard_limit)
  return mdl_hard_limit_;
}
inline void MdlAdagradLightParameters::set_mdl_hard_limit(float value) {
  
  mdl_hard_limit_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.mdl_hard_limit)
}

// bool hard_limit_min_benefit = 11;
inline void MdlAdagradLightParameters::clear_hard_limit_min_benefit() {
  hard_limit_min_benefit_ = false;
}
inline bool MdlAdagradLightParameters::hard_limit_min_benefit() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.hard_limit_min_benefit)
  return hard_limit_min_benefit_;
}
inline void MdlAdagradLightParameters::set_hard_limit_min_benefit(bool value) {
  
  hard_limit_min_benefit_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.hard_limit_min_benefit)
}

// bool mdl_regularize = 12;
inline void MdlAdagradLightParameters::clear_mdl_regularize() {
  mdl_regularize_ = false;
}
inline bool MdlAdagradLightParameters::mdl_regularize() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.MdlAdagradLightParameters.mdl_regularize)
  return mdl_regularize_;
}
inline void MdlAdagradLightParameters::set_mdl_regularize(bool value) {
  
  mdl_regularize_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.MdlAdagradLightParameters.mdl_regularize)
}

// -------------------------------------------------------------------

// AdadeltaParameters

// float rho = 1;
inline void AdadeltaParameters::clear_rho() {
  rho_ = 0;
}
inline float AdadeltaParameters::rho() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdadeltaParameters.rho)
  return rho_;
}
inline void AdadeltaParameters::set_rho(float value) {
  
  rho_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdadeltaParameters.rho)
}

// float epsilon = 2;
inline void AdadeltaParameters::clear_epsilon() {
  epsilon_ = 0;
}
inline float AdadeltaParameters::epsilon() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.AdadeltaParameters.epsilon)
  return epsilon_;
}
inline void AdadeltaParameters::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.AdadeltaParameters.epsilon)
}

// -------------------------------------------------------------------

// ProximalAdagradParameters

// float l1 = 1;
inline void ProximalAdagradParameters::clear_l1() {
  l1_ = 0;
}
inline float ProximalAdagradParameters::l1() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ProximalAdagradParameters.l1)
  return l1_;
}
inline void ProximalAdagradParameters::set_l1(float value) {
  
  l1_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.ProximalAdagradParameters.l1)
}

// float l2 = 2;
inline void ProximalAdagradParameters::clear_l2() {
  l2_ = 0;
}
inline float ProximalAdagradParameters::l2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ProximalAdagradParameters.l2)
  return l2_;
}
inline void ProximalAdagradParameters::set_l2(float value) {
  
  l2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.ProximalAdagradParameters.l2)
}

// -------------------------------------------------------------------

// OnlineYogiParameters

// float l1 = 1;
inline void OnlineYogiParameters::clear_l1() {
  l1_ = 0;
}
inline float OnlineYogiParameters::l1() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OnlineYogiParameters.l1)
  return l1_;
}
inline void OnlineYogiParameters::set_l1(float value) {
  
  l1_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.OnlineYogiParameters.l1)
}

// float l2 = 2;
inline void OnlineYogiParameters::clear_l2() {
  l2_ = 0;
}
inline float OnlineYogiParameters::l2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OnlineYogiParameters.l2)
  return l2_;
}
inline void OnlineYogiParameters::set_l2(float value) {
  
  l2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.OnlineYogiParameters.l2)
}

// float beta2 = 3;
inline void OnlineYogiParameters::clear_beta2() {
  beta2_ = 0;
}
inline float OnlineYogiParameters::beta2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OnlineYogiParameters.beta2)
  return beta2_;
}
inline void OnlineYogiParameters::set_beta2(float value) {
  
  beta2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.OnlineYogiParameters.beta2)
}

// -------------------------------------------------------------------

// ProximalYogiParameters

// float l1 = 1;
inline void ProximalYogiParameters::clear_l1() {
  l1_ = 0;
}
inline float ProximalYogiParameters::l1() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ProximalYogiParameters.l1)
  return l1_;
}
inline void ProximalYogiParameters::set_l1(float value) {
  
  l1_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.ProximalYogiParameters.l1)
}

// float l2 = 2;
inline void ProximalYogiParameters::clear_l2() {
  l2_ = 0;
}
inline float ProximalYogiParameters::l2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ProximalYogiParameters.l2)
  return l2_;
}
inline void ProximalYogiParameters::set_l2(float value) {
  
  l2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.ProximalYogiParameters.l2)
}

// float beta1 = 3;
inline void ProximalYogiParameters::clear_beta1() {
  beta1_ = 0;
}
inline float ProximalYogiParameters::beta1() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ProximalYogiParameters.beta1)
  return beta1_;
}
inline void ProximalYogiParameters::set_beta1(float value) {
  
  beta1_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.ProximalYogiParameters.beta1)
}

// float beta2 = 4;
inline void ProximalYogiParameters::clear_beta2() {
  beta2_ = 0;
}
inline float ProximalYogiParameters::beta2() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ProximalYogiParameters.beta2)
  return beta2_;
}
inline void ProximalYogiParameters::set_beta2(float value) {
  
  beta2_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.ProximalYogiParameters.beta2)
}

// float epsilon = 5;
inline void ProximalYogiParameters::clear_epsilon() {
  epsilon_ = 0;
}
inline float ProximalYogiParameters::epsilon() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.ProximalYogiParameters.epsilon)
  return epsilon_;
}
inline void ProximalYogiParameters::set_epsilon(float value) {
  
  epsilon_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.ProximalYogiParameters.epsilon)
}

// -------------------------------------------------------------------

// FrequencyEstimatorParameters

// float tau = 1;
inline void FrequencyEstimatorParameters::clear_tau() {
  tau_ = 0;
}
inline float FrequencyEstimatorParameters::tau() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FrequencyEstimatorParameters.tau)
  return tau_;
}
inline void FrequencyEstimatorParameters::set_tau(float value) {
  
  tau_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FrequencyEstimatorParameters.tau)
}

// float max_delta = 2;
inline void FrequencyEstimatorParameters::clear_max_delta() {
  max_delta_ = 0;
}
inline float FrequencyEstimatorParameters::max_delta() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FrequencyEstimatorParameters.max_delta)
  return max_delta_;
}
inline void FrequencyEstimatorParameters::set_max_delta(float value) {
  
  max_delta_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FrequencyEstimatorParameters.max_delta)
}

// float outlier_threshold = 3;
inline void FrequencyEstimatorParameters::clear_outlier_threshold() {
  outlier_threshold_ = 0;
}
inline float FrequencyEstimatorParameters::outlier_threshold() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FrequencyEstimatorParameters.outlier_threshold)
  return outlier_threshold_;
}
inline void FrequencyEstimatorParameters::set_outlier_threshold(float value) {
  
  outlier_threshold_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FrequencyEstimatorParameters.outlier_threshold)
}

// float weight_exponent = 4;
inline void FrequencyEstimatorParameters::clear_weight_exponent() {
  weight_exponent_ = 0;
}
inline float FrequencyEstimatorParameters::weight_exponent() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.FrequencyEstimatorParameters.weight_exponent)
  return weight_exponent_;
}
inline void FrequencyEstimatorParameters::set_weight_exponent(float value) {
  
  weight_exponent_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.FrequencyEstimatorParameters.weight_exponent)
}

// -------------------------------------------------------------------

// UserDefinedProgramParameters

// .xla.HloModuleProto program = 1;
inline bool UserDefinedProgramParameters::has_program() const {
  return this != internal_default_instance() && program_ != NULL;
}
inline const ::xla::HloModuleProto& UserDefinedProgramParameters::_internal_program() const {
  return *program_;
}
inline const ::xla::HloModuleProto& UserDefinedProgramParameters::program() const {
  const ::xla::HloModuleProto* p = program_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.UserDefinedProgramParameters.program)
  return p != NULL ? *p : *reinterpret_cast<const ::xla::HloModuleProto*>(
      &::xla::_HloModuleProto_default_instance_);
}
inline ::xla::HloModuleProto* UserDefinedProgramParameters::release_program() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.UserDefinedProgramParameters.program)
  
  ::xla::HloModuleProto* temp = program_;
  program_ = NULL;
  return temp;
}
inline ::xla::HloModuleProto* UserDefinedProgramParameters::mutable_program() {
  
  if (program_ == NULL) {
    auto* p = CreateMaybeMessage<::xla::HloModuleProto>(GetArenaNoVirtual());
    program_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.UserDefinedProgramParameters.program)
  return program_;
}
inline void UserDefinedProgramParameters::set_allocated_program(::xla::HloModuleProto* program) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(program_);
  }
  if (program) {
    ::google::protobuf::Arena* submessage_arena =
      reinterpret_cast<::google::protobuf::MessageLite*>(program)->GetArena();
    if (message_arena != submessage_arena) {
      program = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, program, submessage_arena);
    }
    
  } else {
    
  }
  program_ = program;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.UserDefinedProgramParameters.program)
}

// -------------------------------------------------------------------

// AssignParameters

// -------------------------------------------------------------------

// GradientAccumulationStatus

// -------------------------------------------------------------------

// LowDimensionalPackingStatus

// -------------------------------------------------------------------

// HotIdReplicationConfiguration

// .tensorflow.tpu.HotIdReplicationConfiguration.Status status = 1;
inline void HotIdReplicationConfiguration::clear_status() {
  status_ = 0;
}
inline ::tensorflow::tpu::HotIdReplicationConfiguration_Status HotIdReplicationConfiguration::status() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.HotIdReplicationConfiguration.status)
  return static_cast< ::tensorflow::tpu::HotIdReplicationConfiguration_Status >(status_);
}
inline void HotIdReplicationConfiguration::set_status(::tensorflow::tpu::HotIdReplicationConfiguration_Status value) {
  
  status_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.HotIdReplicationConfiguration.status)
}

// -------------------------------------------------------------------

// OptimizationParameters

// .tensorflow.tpu.LearningRate learning_rate = 13;
inline bool OptimizationParameters::has_learning_rate() const {
  return this != internal_default_instance() && learning_rate_ != NULL;
}
inline void OptimizationParameters::clear_learning_rate() {
  if (GetArenaNoVirtual() == NULL && learning_rate_ != NULL) {
    delete learning_rate_;
  }
  learning_rate_ = NULL;
}
inline const ::tensorflow::tpu::LearningRate& OptimizationParameters::_internal_learning_rate() const {
  return *learning_rate_;
}
inline const ::tensorflow::tpu::LearningRate& OptimizationParameters::learning_rate() const {
  const ::tensorflow::tpu::LearningRate* p = learning_rate_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.learning_rate)
  return p != NULL ? *p : *reinterpret_cast<const ::tensorflow::tpu::LearningRate*>(
      &::tensorflow::tpu::_LearningRate_default_instance_);
}
inline ::tensorflow::tpu::LearningRate* OptimizationParameters::release_learning_rate() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.learning_rate)
  
  ::tensorflow::tpu::LearningRate* temp = learning_rate_;
  learning_rate_ = NULL;
  return temp;
}
inline ::tensorflow::tpu::LearningRate* OptimizationParameters::mutable_learning_rate() {
  
  if (learning_rate_ == NULL) {
    auto* p = CreateMaybeMessage<::tensorflow::tpu::LearningRate>(GetArenaNoVirtual());
    learning_rate_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.learning_rate)
  return learning_rate_;
}
inline void OptimizationParameters::set_allocated_learning_rate(::tensorflow::tpu::LearningRate* learning_rate) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete learning_rate_;
  }
  if (learning_rate) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      learning_rate = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, learning_rate, submessage_arena);
    }
    
  } else {
    
  }
  learning_rate_ = learning_rate;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.OptimizationParameters.learning_rate)
}

// .tensorflow.tpu.ClippingLimits clipping_limits = 2;
inline bool OptimizationParameters::has_clipping_limits() const {
  return this != internal_default_instance() && clipping_limits_ != NULL;
}
inline void OptimizationParameters::clear_clipping_limits() {
  if (GetArenaNoVirtual() == NULL && clipping_limits_ != NULL) {
    delete clipping_limits_;
  }
  clipping_limits_ = NULL;
}
inline const ::tensorflow::tpu::ClippingLimits& OptimizationParameters::_internal_clipping_limits() const {
  return *clipping_limits_;
}
inline const ::tensorflow::tpu::ClippingLimits& OptimizationParameters::clipping_limits() const {
  const ::tensorflow::tpu::ClippingLimits* p = clipping_limits_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.clipping_limits)
  return p != NULL ? *p : *reinterpret_cast<const ::tensorflow::tpu::ClippingLimits*>(
      &::tensorflow::tpu::_ClippingLimits_default_instance_);
}
inline ::tensorflow::tpu::ClippingLimits* OptimizationParameters::release_clipping_limits() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.clipping_limits)
  
  ::tensorflow::tpu::ClippingLimits* temp = clipping_limits_;
  clipping_limits_ = NULL;
  return temp;
}
inline ::tensorflow::tpu::ClippingLimits* OptimizationParameters::mutable_clipping_limits() {
  
  if (clipping_limits_ == NULL) {
    auto* p = CreateMaybeMessage<::tensorflow::tpu::ClippingLimits>(GetArenaNoVirtual());
    clipping_limits_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.clipping_limits)
  return clipping_limits_;
}
inline void OptimizationParameters::set_allocated_clipping_limits(::tensorflow::tpu::ClippingLimits* clipping_limits) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete clipping_limits_;
  }
  if (clipping_limits) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      clipping_limits = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, clipping_limits, submessage_arena);
    }
    
  } else {
    
  }
  clipping_limits_ = clipping_limits;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.OptimizationParameters.clipping_limits)
}

// .tensorflow.tpu.ClippingLimits gradient_clipping_limits = 7;
inline bool OptimizationParameters::has_gradient_clipping_limits() const {
  return this != internal_default_instance() && gradient_clipping_limits_ != NULL;
}
inline void OptimizationParameters::clear_gradient_clipping_limits() {
  if (GetArenaNoVirtual() == NULL && gradient_clipping_limits_ != NULL) {
    delete gradient_clipping_limits_;
  }
  gradient_clipping_limits_ = NULL;
}
inline const ::tensorflow::tpu::ClippingLimits& OptimizationParameters::_internal_gradient_clipping_limits() const {
  return *gradient_clipping_limits_;
}
inline const ::tensorflow::tpu::ClippingLimits& OptimizationParameters::gradient_clipping_limits() const {
  const ::tensorflow::tpu::ClippingLimits* p = gradient_clipping_limits_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.gradient_clipping_limits)
  return p != NULL ? *p : *reinterpret_cast<const ::tensorflow::tpu::ClippingLimits*>(
      &::tensorflow::tpu::_ClippingLimits_default_instance_);
}
inline ::tensorflow::tpu::ClippingLimits* OptimizationParameters::release_gradient_clipping_limits() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.gradient_clipping_limits)
  
  ::tensorflow::tpu::ClippingLimits* temp = gradient_clipping_limits_;
  gradient_clipping_limits_ = NULL;
  return temp;
}
inline ::tensorflow::tpu::ClippingLimits* OptimizationParameters::mutable_gradient_clipping_limits() {
  
  if (gradient_clipping_limits_ == NULL) {
    auto* p = CreateMaybeMessage<::tensorflow::tpu::ClippingLimits>(GetArenaNoVirtual());
    gradient_clipping_limits_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.gradient_clipping_limits)
  return gradient_clipping_limits_;
}
inline void OptimizationParameters::set_allocated_gradient_clipping_limits(::tensorflow::tpu::ClippingLimits* gradient_clipping_limits) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete gradient_clipping_limits_;
  }
  if (gradient_clipping_limits) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      gradient_clipping_limits = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, gradient_clipping_limits, submessage_arena);
    }
    
  } else {
    
  }
  gradient_clipping_limits_ = gradient_clipping_limits;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.OptimizationParameters.gradient_clipping_limits)
}

// float weight_decay_factor = 16;
inline void OptimizationParameters::clear_weight_decay_factor() {
  weight_decay_factor_ = 0;
}
inline float OptimizationParameters::weight_decay_factor() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.weight_decay_factor)
  return weight_decay_factor_;
}
inline void OptimizationParameters::set_weight_decay_factor(float value) {
  
  weight_decay_factor_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.OptimizationParameters.weight_decay_factor)
}

// bool multiply_weight_decay_factor_by_learning_rate = 22;
inline void OptimizationParameters::clear_multiply_weight_decay_factor_by_learning_rate() {
  multiply_weight_decay_factor_by_learning_rate_ = false;
}
inline bool OptimizationParameters::multiply_weight_decay_factor_by_learning_rate() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.multiply_weight_decay_factor_by_learning_rate)
  return multiply_weight_decay_factor_by_learning_rate_;
}
inline void OptimizationParameters::set_multiply_weight_decay_factor_by_learning_rate(bool value) {
  
  multiply_weight_decay_factor_by_learning_rate_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.OptimizationParameters.multiply_weight_decay_factor_by_learning_rate)
}

// .tensorflow.tpu.SimulatedQuantization simulated_quantization = 27;
inline bool OptimizationParameters::has_simulated_quantization() const {
  return this != internal_default_instance() && simulated_quantization_ != NULL;
}
inline void OptimizationParameters::clear_simulated_quantization() {
  if (GetArenaNoVirtual() == NULL && simulated_quantization_ != NULL) {
    delete simulated_quantization_;
  }
  simulated_quantization_ = NULL;
}
inline const ::tensorflow::tpu::SimulatedQuantization& OptimizationParameters::_internal_simulated_quantization() const {
  return *simulated_quantization_;
}
inline const ::tensorflow::tpu::SimulatedQuantization& OptimizationParameters::simulated_quantization() const {
  const ::tensorflow::tpu::SimulatedQuantization* p = simulated_quantization_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.simulated_quantization)
  return p != NULL ? *p : *reinterpret_cast<const ::tensorflow::tpu::SimulatedQuantization*>(
      &::tensorflow::tpu::_SimulatedQuantization_default_instance_);
}
inline ::tensorflow::tpu::SimulatedQuantization* OptimizationParameters::release_simulated_quantization() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.simulated_quantization)
  
  ::tensorflow::tpu::SimulatedQuantization* temp = simulated_quantization_;
  simulated_quantization_ = NULL;
  return temp;
}
inline ::tensorflow::tpu::SimulatedQuantization* OptimizationParameters::mutable_simulated_quantization() {
  
  if (simulated_quantization_ == NULL) {
    auto* p = CreateMaybeMessage<::tensorflow::tpu::SimulatedQuantization>(GetArenaNoVirtual());
    simulated_quantization_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.simulated_quantization)
  return simulated_quantization_;
}
inline void OptimizationParameters::set_allocated_simulated_quantization(::tensorflow::tpu::SimulatedQuantization* simulated_quantization) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete simulated_quantization_;
  }
  if (simulated_quantization) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      simulated_quantization = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, simulated_quantization, submessage_arena);
    }
    
  } else {
    
  }
  simulated_quantization_ = simulated_quantization;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.OptimizationParameters.simulated_quantization)
}

// .tensorflow.tpu.GradientAccumulationStatus.Status gradient_accumulation_status = 17;
inline void OptimizationParameters::clear_gradient_accumulation_status() {
  gradient_accumulation_status_ = 0;
}
inline ::tensorflow::tpu::GradientAccumulationStatus_Status OptimizationParameters::gradient_accumulation_status() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.gradient_accumulation_status)
  return static_cast< ::tensorflow::tpu::GradientAccumulationStatus_Status >(gradient_accumulation_status_);
}
inline void OptimizationParameters::set_gradient_accumulation_status(::tensorflow::tpu::GradientAccumulationStatus_Status value) {
  
  gradient_accumulation_status_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.OptimizationParameters.gradient_accumulation_status)
}

// .tensorflow.tpu.LowDimensionalPackingStatus.Status low_dimensional_packing_status = 28;
inline void OptimizationParameters::clear_low_dimensional_packing_status() {
  low_dimensional_packing_status_ = 0;
}
inline ::tensorflow::tpu::LowDimensionalPackingStatus_Status OptimizationParameters::low_dimensional_packing_status() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.low_dimensional_packing_status)
  return static_cast< ::tensorflow::tpu::LowDimensionalPackingStatus_Status >(low_dimensional_packing_status_);
}
inline void OptimizationParameters::set_low_dimensional_packing_status(::tensorflow::tpu::LowDimensionalPackingStatus_Status value) {
  
  low_dimensional_packing_status_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.OptimizationParameters.low_dimensional_packing_status)
}

// .tensorflow.tpu.HotIdReplicationConfiguration hot_id_replication_configuration = 18;
inline bool OptimizationParameters::has_hot_id_replication_configuration() const {
  return this != internal_default_instance() && hot_id_replication_configuration_ != NULL;
}
inline void OptimizationParameters::clear_hot_id_replication_configuration() {
  if (GetArenaNoVirtual() == NULL && hot_id_replication_configuration_ != NULL) {
    delete hot_id_replication_configuration_;
  }
  hot_id_replication_configuration_ = NULL;
}
inline const ::tensorflow::tpu::HotIdReplicationConfiguration& OptimizationParameters::_internal_hot_id_replication_configuration() const {
  return *hot_id_replication_configuration_;
}
inline const ::tensorflow::tpu::HotIdReplicationConfiguration& OptimizationParameters::hot_id_replication_configuration() const {
  const ::tensorflow::tpu::HotIdReplicationConfiguration* p = hot_id_replication_configuration_;
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.hot_id_replication_configuration)
  return p != NULL ? *p : *reinterpret_cast<const ::tensorflow::tpu::HotIdReplicationConfiguration*>(
      &::tensorflow::tpu::_HotIdReplicationConfiguration_default_instance_);
}
inline ::tensorflow::tpu::HotIdReplicationConfiguration* OptimizationParameters::release_hot_id_replication_configuration() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.hot_id_replication_configuration)
  
  ::tensorflow::tpu::HotIdReplicationConfiguration* temp = hot_id_replication_configuration_;
  hot_id_replication_configuration_ = NULL;
  return temp;
}
inline ::tensorflow::tpu::HotIdReplicationConfiguration* OptimizationParameters::mutable_hot_id_replication_configuration() {
  
  if (hot_id_replication_configuration_ == NULL) {
    auto* p = CreateMaybeMessage<::tensorflow::tpu::HotIdReplicationConfiguration>(GetArenaNoVirtual());
    hot_id_replication_configuration_ = p;
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.hot_id_replication_configuration)
  return hot_id_replication_configuration_;
}
inline void OptimizationParameters::set_allocated_hot_id_replication_configuration(::tensorflow::tpu::HotIdReplicationConfiguration* hot_id_replication_configuration) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete hot_id_replication_configuration_;
  }
  if (hot_id_replication_configuration) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      hot_id_replication_configuration = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, hot_id_replication_configuration, submessage_arena);
    }
    
  } else {
    
  }
  hot_id_replication_configuration_ = hot_id_replication_configuration;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.OptimizationParameters.hot_id_replication_configuration)
}

// .tensorflow.tpu.AdagradParameters adagrad = 3;
inline bool OptimizationParameters::has_adagrad() const {
  return parameters_case() == kAdagrad;
}
inline void OptimizationParameters::set_has_adagrad() {
  _oneof_case_[0] = kAdagrad;
}
inline void OptimizationParameters::clear_adagrad() {
  if (has_adagrad()) {
    delete parameters_.adagrad_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::AdagradParameters& OptimizationParameters::_internal_adagrad() const {
  return *parameters_.adagrad_;
}
inline ::tensorflow::tpu::AdagradParameters* OptimizationParameters::release_adagrad() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.adagrad)
  if (has_adagrad()) {
    clear_has_parameters();
      ::tensorflow::tpu::AdagradParameters* temp = parameters_.adagrad_;
    parameters_.adagrad_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::AdagradParameters& OptimizationParameters::adagrad() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.adagrad)
  return has_adagrad()
      ? *parameters_.adagrad_
      : *reinterpret_cast< ::tensorflow::tpu::AdagradParameters*>(&::tensorflow::tpu::_AdagradParameters_default_instance_);
}
inline ::tensorflow::tpu::AdagradParameters* OptimizationParameters::mutable_adagrad() {
  if (!has_adagrad()) {
    clear_parameters();
    set_has_adagrad();
    parameters_.adagrad_ = CreateMaybeMessage< ::tensorflow::tpu::AdagradParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.adagrad)
  return parameters_.adagrad_;
}

// .tensorflow.tpu.AdagradMomentumParameters adagrad_momentum = 26;
inline bool OptimizationParameters::has_adagrad_momentum() const {
  return parameters_case() == kAdagradMomentum;
}
inline void OptimizationParameters::set_has_adagrad_momentum() {
  _oneof_case_[0] = kAdagradMomentum;
}
inline void OptimizationParameters::clear_adagrad_momentum() {
  if (has_adagrad_momentum()) {
    delete parameters_.adagrad_momentum_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::AdagradMomentumParameters& OptimizationParameters::_internal_adagrad_momentum() const {
  return *parameters_.adagrad_momentum_;
}
inline ::tensorflow::tpu::AdagradMomentumParameters* OptimizationParameters::release_adagrad_momentum() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.adagrad_momentum)
  if (has_adagrad_momentum()) {
    clear_has_parameters();
      ::tensorflow::tpu::AdagradMomentumParameters* temp = parameters_.adagrad_momentum_;
    parameters_.adagrad_momentum_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::AdagradMomentumParameters& OptimizationParameters::adagrad_momentum() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.adagrad_momentum)
  return has_adagrad_momentum()
      ? *parameters_.adagrad_momentum_
      : *reinterpret_cast< ::tensorflow::tpu::AdagradMomentumParameters*>(&::tensorflow::tpu::_AdagradMomentumParameters_default_instance_);
}
inline ::tensorflow::tpu::AdagradMomentumParameters* OptimizationParameters::mutable_adagrad_momentum() {
  if (!has_adagrad_momentum()) {
    clear_parameters();
    set_has_adagrad_momentum();
    parameters_.adagrad_momentum_ = CreateMaybeMessage< ::tensorflow::tpu::AdagradMomentumParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.adagrad_momentum)
  return parameters_.adagrad_momentum_;
}

// .tensorflow.tpu.BoundedAdagradParameters bounded_adagrad = 19;
inline bool OptimizationParameters::has_bounded_adagrad() const {
  return parameters_case() == kBoundedAdagrad;
}
inline void OptimizationParameters::set_has_bounded_adagrad() {
  _oneof_case_[0] = kBoundedAdagrad;
}
inline void OptimizationParameters::clear_bounded_adagrad() {
  if (has_bounded_adagrad()) {
    delete parameters_.bounded_adagrad_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::BoundedAdagradParameters& OptimizationParameters::_internal_bounded_adagrad() const {
  return *parameters_.bounded_adagrad_;
}
inline ::tensorflow::tpu::BoundedAdagradParameters* OptimizationParameters::release_bounded_adagrad() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.bounded_adagrad)
  if (has_bounded_adagrad()) {
    clear_has_parameters();
      ::tensorflow::tpu::BoundedAdagradParameters* temp = parameters_.bounded_adagrad_;
    parameters_.bounded_adagrad_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::BoundedAdagradParameters& OptimizationParameters::bounded_adagrad() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.bounded_adagrad)
  return has_bounded_adagrad()
      ? *parameters_.bounded_adagrad_
      : *reinterpret_cast< ::tensorflow::tpu::BoundedAdagradParameters*>(&::tensorflow::tpu::_BoundedAdagradParameters_default_instance_);
}
inline ::tensorflow::tpu::BoundedAdagradParameters* OptimizationParameters::mutable_bounded_adagrad() {
  if (!has_bounded_adagrad()) {
    clear_parameters();
    set_has_bounded_adagrad();
    parameters_.bounded_adagrad_ = CreateMaybeMessage< ::tensorflow::tpu::BoundedAdagradParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.bounded_adagrad)
  return parameters_.bounded_adagrad_;
}

// .tensorflow.tpu.StochasticGradientDescentParameters stochastic_gradient_descent = 4;
inline bool OptimizationParameters::has_stochastic_gradient_descent() const {
  return parameters_case() == kStochasticGradientDescent;
}
inline void OptimizationParameters::set_has_stochastic_gradient_descent() {
  _oneof_case_[0] = kStochasticGradientDescent;
}
inline void OptimizationParameters::clear_stochastic_gradient_descent() {
  if (has_stochastic_gradient_descent()) {
    delete parameters_.stochastic_gradient_descent_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::StochasticGradientDescentParameters& OptimizationParameters::_internal_stochastic_gradient_descent() const {
  return *parameters_.stochastic_gradient_descent_;
}
inline ::tensorflow::tpu::StochasticGradientDescentParameters* OptimizationParameters::release_stochastic_gradient_descent() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.stochastic_gradient_descent)
  if (has_stochastic_gradient_descent()) {
    clear_has_parameters();
      ::tensorflow::tpu::StochasticGradientDescentParameters* temp = parameters_.stochastic_gradient_descent_;
    parameters_.stochastic_gradient_descent_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::StochasticGradientDescentParameters& OptimizationParameters::stochastic_gradient_descent() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.stochastic_gradient_descent)
  return has_stochastic_gradient_descent()
      ? *parameters_.stochastic_gradient_descent_
      : *reinterpret_cast< ::tensorflow::tpu::StochasticGradientDescentParameters*>(&::tensorflow::tpu::_StochasticGradientDescentParameters_default_instance_);
}
inline ::tensorflow::tpu::StochasticGradientDescentParameters* OptimizationParameters::mutable_stochastic_gradient_descent() {
  if (!has_stochastic_gradient_descent()) {
    clear_parameters();
    set_has_stochastic_gradient_descent();
    parameters_.stochastic_gradient_descent_ = CreateMaybeMessage< ::tensorflow::tpu::StochasticGradientDescentParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.stochastic_gradient_descent)
  return parameters_.stochastic_gradient_descent_;
}

// .tensorflow.tpu.FtrlParameters ftrl = 5;
inline bool OptimizationParameters::has_ftrl() const {
  return parameters_case() == kFtrl;
}
inline void OptimizationParameters::set_has_ftrl() {
  _oneof_case_[0] = kFtrl;
}
inline void OptimizationParameters::clear_ftrl() {
  if (has_ftrl()) {
    delete parameters_.ftrl_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::FtrlParameters& OptimizationParameters::_internal_ftrl() const {
  return *parameters_.ftrl_;
}
inline ::tensorflow::tpu::FtrlParameters* OptimizationParameters::release_ftrl() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.ftrl)
  if (has_ftrl()) {
    clear_has_parameters();
      ::tensorflow::tpu::FtrlParameters* temp = parameters_.ftrl_;
    parameters_.ftrl_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::FtrlParameters& OptimizationParameters::ftrl() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.ftrl)
  return has_ftrl()
      ? *parameters_.ftrl_
      : *reinterpret_cast< ::tensorflow::tpu::FtrlParameters*>(&::tensorflow::tpu::_FtrlParameters_default_instance_);
}
inline ::tensorflow::tpu::FtrlParameters* OptimizationParameters::mutable_ftrl() {
  if (!has_ftrl()) {
    clear_parameters();
    set_has_ftrl();
    parameters_.ftrl_ = CreateMaybeMessage< ::tensorflow::tpu::FtrlParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.ftrl)
  return parameters_.ftrl_;
}

// .tensorflow.tpu.AdamParameters adam = 6;
inline bool OptimizationParameters::has_adam() const {
  return parameters_case() == kAdam;
}
inline void OptimizationParameters::set_has_adam() {
  _oneof_case_[0] = kAdam;
}
inline void OptimizationParameters::clear_adam() {
  if (has_adam()) {
    delete parameters_.adam_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::AdamParameters& OptimizationParameters::_internal_adam() const {
  return *parameters_.adam_;
}
inline ::tensorflow::tpu::AdamParameters* OptimizationParameters::release_adam() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.adam)
  if (has_adam()) {
    clear_has_parameters();
      ::tensorflow::tpu::AdamParameters* temp = parameters_.adam_;
    parameters_.adam_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::AdamParameters& OptimizationParameters::adam() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.adam)
  return has_adam()
      ? *parameters_.adam_
      : *reinterpret_cast< ::tensorflow::tpu::AdamParameters*>(&::tensorflow::tpu::_AdamParameters_default_instance_);
}
inline ::tensorflow::tpu::AdamParameters* OptimizationParameters::mutable_adam() {
  if (!has_adam()) {
    clear_parameters();
    set_has_adam();
    parameters_.adam_ = CreateMaybeMessage< ::tensorflow::tpu::AdamParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.adam)
  return parameters_.adam_;
}

// .tensorflow.tpu.MomentumParameters momentum = 8;
inline bool OptimizationParameters::has_momentum() const {
  return parameters_case() == kMomentum;
}
inline void OptimizationParameters::set_has_momentum() {
  _oneof_case_[0] = kMomentum;
}
inline void OptimizationParameters::clear_momentum() {
  if (has_momentum()) {
    delete parameters_.momentum_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::MomentumParameters& OptimizationParameters::_internal_momentum() const {
  return *parameters_.momentum_;
}
inline ::tensorflow::tpu::MomentumParameters* OptimizationParameters::release_momentum() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.momentum)
  if (has_momentum()) {
    clear_has_parameters();
      ::tensorflow::tpu::MomentumParameters* temp = parameters_.momentum_;
    parameters_.momentum_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::MomentumParameters& OptimizationParameters::momentum() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.momentum)
  return has_momentum()
      ? *parameters_.momentum_
      : *reinterpret_cast< ::tensorflow::tpu::MomentumParameters*>(&::tensorflow::tpu::_MomentumParameters_default_instance_);
}
inline ::tensorflow::tpu::MomentumParameters* OptimizationParameters::mutable_momentum() {
  if (!has_momentum()) {
    clear_parameters();
    set_has_momentum();
    parameters_.momentum_ = CreateMaybeMessage< ::tensorflow::tpu::MomentumParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.momentum)
  return parameters_.momentum_;
}

// .tensorflow.tpu.RmsPropParameters rms_prop = 9;
inline bool OptimizationParameters::has_rms_prop() const {
  return parameters_case() == kRmsProp;
}
inline void OptimizationParameters::set_has_rms_prop() {
  _oneof_case_[0] = kRmsProp;
}
inline void OptimizationParameters::clear_rms_prop() {
  if (has_rms_prop()) {
    delete parameters_.rms_prop_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::RmsPropParameters& OptimizationParameters::_internal_rms_prop() const {
  return *parameters_.rms_prop_;
}
inline ::tensorflow::tpu::RmsPropParameters* OptimizationParameters::release_rms_prop() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.rms_prop)
  if (has_rms_prop()) {
    clear_has_parameters();
      ::tensorflow::tpu::RmsPropParameters* temp = parameters_.rms_prop_;
    parameters_.rms_prop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::RmsPropParameters& OptimizationParameters::rms_prop() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.rms_prop)
  return has_rms_prop()
      ? *parameters_.rms_prop_
      : *reinterpret_cast< ::tensorflow::tpu::RmsPropParameters*>(&::tensorflow::tpu::_RmsPropParameters_default_instance_);
}
inline ::tensorflow::tpu::RmsPropParameters* OptimizationParameters::mutable_rms_prop() {
  if (!has_rms_prop()) {
    clear_parameters();
    set_has_rms_prop();
    parameters_.rms_prop_ = CreateMaybeMessage< ::tensorflow::tpu::RmsPropParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.rms_prop)
  return parameters_.rms_prop_;
}

// .tensorflow.tpu.CenteredRmsPropParameters centered_rms_prop = 10;
inline bool OptimizationParameters::has_centered_rms_prop() const {
  return parameters_case() == kCenteredRmsProp;
}
inline void OptimizationParameters::set_has_centered_rms_prop() {
  _oneof_case_[0] = kCenteredRmsProp;
}
inline void OptimizationParameters::clear_centered_rms_prop() {
  if (has_centered_rms_prop()) {
    delete parameters_.centered_rms_prop_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::CenteredRmsPropParameters& OptimizationParameters::_internal_centered_rms_prop() const {
  return *parameters_.centered_rms_prop_;
}
inline ::tensorflow::tpu::CenteredRmsPropParameters* OptimizationParameters::release_centered_rms_prop() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.centered_rms_prop)
  if (has_centered_rms_prop()) {
    clear_has_parameters();
      ::tensorflow::tpu::CenteredRmsPropParameters* temp = parameters_.centered_rms_prop_;
    parameters_.centered_rms_prop_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::CenteredRmsPropParameters& OptimizationParameters::centered_rms_prop() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.centered_rms_prop)
  return has_centered_rms_prop()
      ? *parameters_.centered_rms_prop_
      : *reinterpret_cast< ::tensorflow::tpu::CenteredRmsPropParameters*>(&::tensorflow::tpu::_CenteredRmsPropParameters_default_instance_);
}
inline ::tensorflow::tpu::CenteredRmsPropParameters* OptimizationParameters::mutable_centered_rms_prop() {
  if (!has_centered_rms_prop()) {
    clear_parameters();
    set_has_centered_rms_prop();
    parameters_.centered_rms_prop_ = CreateMaybeMessage< ::tensorflow::tpu::CenteredRmsPropParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.centered_rms_prop)
  return parameters_.centered_rms_prop_;
}

// .tensorflow.tpu.MdlAdagradLightParameters mdl_adagrad_light = 11;
inline bool OptimizationParameters::has_mdl_adagrad_light() const {
  return parameters_case() == kMdlAdagradLight;
}
inline void OptimizationParameters::set_has_mdl_adagrad_light() {
  _oneof_case_[0] = kMdlAdagradLight;
}
inline void OptimizationParameters::clear_mdl_adagrad_light() {
  if (has_mdl_adagrad_light()) {
    delete parameters_.mdl_adagrad_light_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::MdlAdagradLightParameters& OptimizationParameters::_internal_mdl_adagrad_light() const {
  return *parameters_.mdl_adagrad_light_;
}
inline ::tensorflow::tpu::MdlAdagradLightParameters* OptimizationParameters::release_mdl_adagrad_light() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.mdl_adagrad_light)
  if (has_mdl_adagrad_light()) {
    clear_has_parameters();
      ::tensorflow::tpu::MdlAdagradLightParameters* temp = parameters_.mdl_adagrad_light_;
    parameters_.mdl_adagrad_light_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::MdlAdagradLightParameters& OptimizationParameters::mdl_adagrad_light() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.mdl_adagrad_light)
  return has_mdl_adagrad_light()
      ? *parameters_.mdl_adagrad_light_
      : *reinterpret_cast< ::tensorflow::tpu::MdlAdagradLightParameters*>(&::tensorflow::tpu::_MdlAdagradLightParameters_default_instance_);
}
inline ::tensorflow::tpu::MdlAdagradLightParameters* OptimizationParameters::mutable_mdl_adagrad_light() {
  if (!has_mdl_adagrad_light()) {
    clear_parameters();
    set_has_mdl_adagrad_light();
    parameters_.mdl_adagrad_light_ = CreateMaybeMessage< ::tensorflow::tpu::MdlAdagradLightParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.mdl_adagrad_light)
  return parameters_.mdl_adagrad_light_;
}

// .tensorflow.tpu.AdadeltaParameters adadelta = 12;
inline bool OptimizationParameters::has_adadelta() const {
  return parameters_case() == kAdadelta;
}
inline void OptimizationParameters::set_has_adadelta() {
  _oneof_case_[0] = kAdadelta;
}
inline void OptimizationParameters::clear_adadelta() {
  if (has_adadelta()) {
    delete parameters_.adadelta_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::AdadeltaParameters& OptimizationParameters::_internal_adadelta() const {
  return *parameters_.adadelta_;
}
inline ::tensorflow::tpu::AdadeltaParameters* OptimizationParameters::release_adadelta() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.adadelta)
  if (has_adadelta()) {
    clear_has_parameters();
      ::tensorflow::tpu::AdadeltaParameters* temp = parameters_.adadelta_;
    parameters_.adadelta_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::AdadeltaParameters& OptimizationParameters::adadelta() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.adadelta)
  return has_adadelta()
      ? *parameters_.adadelta_
      : *reinterpret_cast< ::tensorflow::tpu::AdadeltaParameters*>(&::tensorflow::tpu::_AdadeltaParameters_default_instance_);
}
inline ::tensorflow::tpu::AdadeltaParameters* OptimizationParameters::mutable_adadelta() {
  if (!has_adadelta()) {
    clear_parameters();
    set_has_adadelta();
    parameters_.adadelta_ = CreateMaybeMessage< ::tensorflow::tpu::AdadeltaParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.adadelta)
  return parameters_.adadelta_;
}

// .tensorflow.tpu.ProximalAdagradParameters proximal_adagrad = 14;
inline bool OptimizationParameters::has_proximal_adagrad() const {
  return parameters_case() == kProximalAdagrad;
}
inline void OptimizationParameters::set_has_proximal_adagrad() {
  _oneof_case_[0] = kProximalAdagrad;
}
inline void OptimizationParameters::clear_proximal_adagrad() {
  if (has_proximal_adagrad()) {
    delete parameters_.proximal_adagrad_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::ProximalAdagradParameters& OptimizationParameters::_internal_proximal_adagrad() const {
  return *parameters_.proximal_adagrad_;
}
inline ::tensorflow::tpu::ProximalAdagradParameters* OptimizationParameters::release_proximal_adagrad() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.proximal_adagrad)
  if (has_proximal_adagrad()) {
    clear_has_parameters();
      ::tensorflow::tpu::ProximalAdagradParameters* temp = parameters_.proximal_adagrad_;
    parameters_.proximal_adagrad_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::ProximalAdagradParameters& OptimizationParameters::proximal_adagrad() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.proximal_adagrad)
  return has_proximal_adagrad()
      ? *parameters_.proximal_adagrad_
      : *reinterpret_cast< ::tensorflow::tpu::ProximalAdagradParameters*>(&::tensorflow::tpu::_ProximalAdagradParameters_default_instance_);
}
inline ::tensorflow::tpu::ProximalAdagradParameters* OptimizationParameters::mutable_proximal_adagrad() {
  if (!has_proximal_adagrad()) {
    clear_parameters();
    set_has_proximal_adagrad();
    parameters_.proximal_adagrad_ = CreateMaybeMessage< ::tensorflow::tpu::ProximalAdagradParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.proximal_adagrad)
  return parameters_.proximal_adagrad_;
}

// .tensorflow.tpu.OnlineYogiParameters online_yogi = 20;
inline bool OptimizationParameters::has_online_yogi() const {
  return parameters_case() == kOnlineYogi;
}
inline void OptimizationParameters::set_has_online_yogi() {
  _oneof_case_[0] = kOnlineYogi;
}
inline void OptimizationParameters::clear_online_yogi() {
  if (has_online_yogi()) {
    delete parameters_.online_yogi_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::OnlineYogiParameters& OptimizationParameters::_internal_online_yogi() const {
  return *parameters_.online_yogi_;
}
inline ::tensorflow::tpu::OnlineYogiParameters* OptimizationParameters::release_online_yogi() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.online_yogi)
  if (has_online_yogi()) {
    clear_has_parameters();
      ::tensorflow::tpu::OnlineYogiParameters* temp = parameters_.online_yogi_;
    parameters_.online_yogi_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::OnlineYogiParameters& OptimizationParameters::online_yogi() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.online_yogi)
  return has_online_yogi()
      ? *parameters_.online_yogi_
      : *reinterpret_cast< ::tensorflow::tpu::OnlineYogiParameters*>(&::tensorflow::tpu::_OnlineYogiParameters_default_instance_);
}
inline ::tensorflow::tpu::OnlineYogiParameters* OptimizationParameters::mutable_online_yogi() {
  if (!has_online_yogi()) {
    clear_parameters();
    set_has_online_yogi();
    parameters_.online_yogi_ = CreateMaybeMessage< ::tensorflow::tpu::OnlineYogiParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.online_yogi)
  return parameters_.online_yogi_;
}

// .tensorflow.tpu.ProximalYogiParameters proximal_yogi = 21;
inline bool OptimizationParameters::has_proximal_yogi() const {
  return parameters_case() == kProximalYogi;
}
inline void OptimizationParameters::set_has_proximal_yogi() {
  _oneof_case_[0] = kProximalYogi;
}
inline void OptimizationParameters::clear_proximal_yogi() {
  if (has_proximal_yogi()) {
    delete parameters_.proximal_yogi_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::ProximalYogiParameters& OptimizationParameters::_internal_proximal_yogi() const {
  return *parameters_.proximal_yogi_;
}
inline ::tensorflow::tpu::ProximalYogiParameters* OptimizationParameters::release_proximal_yogi() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.proximal_yogi)
  if (has_proximal_yogi()) {
    clear_has_parameters();
      ::tensorflow::tpu::ProximalYogiParameters* temp = parameters_.proximal_yogi_;
    parameters_.proximal_yogi_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::ProximalYogiParameters& OptimizationParameters::proximal_yogi() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.proximal_yogi)
  return has_proximal_yogi()
      ? *parameters_.proximal_yogi_
      : *reinterpret_cast< ::tensorflow::tpu::ProximalYogiParameters*>(&::tensorflow::tpu::_ProximalYogiParameters_default_instance_);
}
inline ::tensorflow::tpu::ProximalYogiParameters* OptimizationParameters::mutable_proximal_yogi() {
  if (!has_proximal_yogi()) {
    clear_parameters();
    set_has_proximal_yogi();
    parameters_.proximal_yogi_ = CreateMaybeMessage< ::tensorflow::tpu::ProximalYogiParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.proximal_yogi)
  return parameters_.proximal_yogi_;
}

// .tensorflow.tpu.FrequencyEstimatorParameters frequency_estimator = 23;
inline bool OptimizationParameters::has_frequency_estimator() const {
  return parameters_case() == kFrequencyEstimator;
}
inline void OptimizationParameters::set_has_frequency_estimator() {
  _oneof_case_[0] = kFrequencyEstimator;
}
inline void OptimizationParameters::clear_frequency_estimator() {
  if (has_frequency_estimator()) {
    delete parameters_.frequency_estimator_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::FrequencyEstimatorParameters& OptimizationParameters::_internal_frequency_estimator() const {
  return *parameters_.frequency_estimator_;
}
inline ::tensorflow::tpu::FrequencyEstimatorParameters* OptimizationParameters::release_frequency_estimator() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.frequency_estimator)
  if (has_frequency_estimator()) {
    clear_has_parameters();
      ::tensorflow::tpu::FrequencyEstimatorParameters* temp = parameters_.frequency_estimator_;
    parameters_.frequency_estimator_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::FrequencyEstimatorParameters& OptimizationParameters::frequency_estimator() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.frequency_estimator)
  return has_frequency_estimator()
      ? *parameters_.frequency_estimator_
      : *reinterpret_cast< ::tensorflow::tpu::FrequencyEstimatorParameters*>(&::tensorflow::tpu::_FrequencyEstimatorParameters_default_instance_);
}
inline ::tensorflow::tpu::FrequencyEstimatorParameters* OptimizationParameters::mutable_frequency_estimator() {
  if (!has_frequency_estimator()) {
    clear_parameters();
    set_has_frequency_estimator();
    parameters_.frequency_estimator_ = CreateMaybeMessage< ::tensorflow::tpu::FrequencyEstimatorParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.frequency_estimator)
  return parameters_.frequency_estimator_;
}

// .tensorflow.tpu.UserDefinedProgramParameters user_defined_program = 24;
inline bool OptimizationParameters::has_user_defined_program() const {
  return parameters_case() == kUserDefinedProgram;
}
inline void OptimizationParameters::set_has_user_defined_program() {
  _oneof_case_[0] = kUserDefinedProgram;
}
inline void OptimizationParameters::clear_user_defined_program() {
  if (has_user_defined_program()) {
    delete parameters_.user_defined_program_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::UserDefinedProgramParameters& OptimizationParameters::_internal_user_defined_program() const {
  return *parameters_.user_defined_program_;
}
inline ::tensorflow::tpu::UserDefinedProgramParameters* OptimizationParameters::release_user_defined_program() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.user_defined_program)
  if (has_user_defined_program()) {
    clear_has_parameters();
      ::tensorflow::tpu::UserDefinedProgramParameters* temp = parameters_.user_defined_program_;
    parameters_.user_defined_program_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::UserDefinedProgramParameters& OptimizationParameters::user_defined_program() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.user_defined_program)
  return has_user_defined_program()
      ? *parameters_.user_defined_program_
      : *reinterpret_cast< ::tensorflow::tpu::UserDefinedProgramParameters*>(&::tensorflow::tpu::_UserDefinedProgramParameters_default_instance_);
}
inline ::tensorflow::tpu::UserDefinedProgramParameters* OptimizationParameters::mutable_user_defined_program() {
  if (!has_user_defined_program()) {
    clear_parameters();
    set_has_user_defined_program();
    parameters_.user_defined_program_ = CreateMaybeMessage< ::tensorflow::tpu::UserDefinedProgramParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.user_defined_program)
  return parameters_.user_defined_program_;
}

// .tensorflow.tpu.AssignParameters assign = 25;
inline bool OptimizationParameters::has_assign() const {
  return parameters_case() == kAssign;
}
inline void OptimizationParameters::set_has_assign() {
  _oneof_case_[0] = kAssign;
}
inline void OptimizationParameters::clear_assign() {
  if (has_assign()) {
    delete parameters_.assign_;
    clear_has_parameters();
  }
}
inline const ::tensorflow::tpu::AssignParameters& OptimizationParameters::_internal_assign() const {
  return *parameters_.assign_;
}
inline ::tensorflow::tpu::AssignParameters* OptimizationParameters::release_assign() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.OptimizationParameters.assign)
  if (has_assign()) {
    clear_has_parameters();
      ::tensorflow::tpu::AssignParameters* temp = parameters_.assign_;
    parameters_.assign_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::AssignParameters& OptimizationParameters::assign() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.OptimizationParameters.assign)
  return has_assign()
      ? *parameters_.assign_
      : *reinterpret_cast< ::tensorflow::tpu::AssignParameters*>(&::tensorflow::tpu::_AssignParameters_default_instance_);
}
inline ::tensorflow::tpu::AssignParameters* OptimizationParameters::mutable_assign() {
  if (!has_assign()) {
    clear_parameters();
    set_has_assign();
    parameters_.assign_ = CreateMaybeMessage< ::tensorflow::tpu::AssignParameters >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.OptimizationParameters.assign)
  return parameters_.assign_;
}

inline bool OptimizationParameters::has_parameters() const {
  return parameters_case() != PARAMETERS_NOT_SET;
}
inline void OptimizationParameters::clear_has_parameters() {
  _oneof_case_[0] = PARAMETERS_NOT_SET;
}
inline OptimizationParameters::ParametersCase OptimizationParameters::parameters_case() const {
  return OptimizationParameters::ParametersCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// StateVariableSpecification_UserDefined

// -------------------------------------------------------------------

// StateVariableSpecification_FillWithConstant

// double initial_value = 1;
inline void StateVariableSpecification_FillWithConstant::clear_initial_value() {
  initial_value_ = 0;
}
inline double StateVariableSpecification_FillWithConstant::initial_value() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.StateVariableSpecification.FillWithConstant.initial_value)
  return initial_value_;
}
inline void StateVariableSpecification_FillWithConstant::set_initial_value(double value) {
  
  initial_value_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.tpu.StateVariableSpecification.FillWithConstant.initial_value)
}

// -------------------------------------------------------------------

// StateVariableSpecification

// string name = 1;
inline void StateVariableSpecification::clear_name() {
  name_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline const ::std::string& StateVariableSpecification::name() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.StateVariableSpecification.name)
  return name_.GetNoArena();
}
inline void StateVariableSpecification::set_name(const ::std::string& value) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:tensorflow.tpu.StateVariableSpecification.name)
}
#if LANG_CXX11
inline void StateVariableSpecification::set_name(::std::string&& value) {
  
  name_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:tensorflow.tpu.StateVariableSpecification.name)
}
#endif
inline void StateVariableSpecification::set_name(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:tensorflow.tpu.StateVariableSpecification.name)
}
inline void StateVariableSpecification::set_name(const char* value, size_t size) {
  
  name_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:tensorflow.tpu.StateVariableSpecification.name)
}
inline ::std::string* StateVariableSpecification::mutable_name() {
  
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.StateVariableSpecification.name)
  return name_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* StateVariableSpecification::release_name() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.StateVariableSpecification.name)
  
  return name_.ReleaseNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void StateVariableSpecification::set_allocated_name(::std::string* name) {
  if (name != NULL) {
    
  } else {
    
  }
  name_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), name);
  // @@protoc_insertion_point(field_set_allocated:tensorflow.tpu.StateVariableSpecification.name)
}

// .tensorflow.tpu.StateVariableSpecification.UserDefined user_defined = 2;
inline bool StateVariableSpecification::has_user_defined() const {
  return usage_case() == kUserDefined;
}
inline void StateVariableSpecification::set_has_user_defined() {
  _oneof_case_[0] = kUserDefined;
}
inline void StateVariableSpecification::clear_user_defined() {
  if (has_user_defined()) {
    delete usage_.user_defined_;
    clear_has_usage();
  }
}
inline const ::tensorflow::tpu::StateVariableSpecification_UserDefined& StateVariableSpecification::_internal_user_defined() const {
  return *usage_.user_defined_;
}
inline ::tensorflow::tpu::StateVariableSpecification_UserDefined* StateVariableSpecification::release_user_defined() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.StateVariableSpecification.user_defined)
  if (has_user_defined()) {
    clear_has_usage();
      ::tensorflow::tpu::StateVariableSpecification_UserDefined* temp = usage_.user_defined_;
    usage_.user_defined_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::StateVariableSpecification_UserDefined& StateVariableSpecification::user_defined() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.StateVariableSpecification.user_defined)
  return has_user_defined()
      ? *usage_.user_defined_
      : *reinterpret_cast< ::tensorflow::tpu::StateVariableSpecification_UserDefined*>(&::tensorflow::tpu::_StateVariableSpecification_UserDefined_default_instance_);
}
inline ::tensorflow::tpu::StateVariableSpecification_UserDefined* StateVariableSpecification::mutable_user_defined() {
  if (!has_user_defined()) {
    clear_usage();
    set_has_user_defined();
    usage_.user_defined_ = CreateMaybeMessage< ::tensorflow::tpu::StateVariableSpecification_UserDefined >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.StateVariableSpecification.user_defined)
  return usage_.user_defined_;
}

// .tensorflow.tpu.StateVariableSpecification.FillWithConstant fill_with_constant = 3;
inline bool StateVariableSpecification::has_fill_with_constant() const {
  return usage_case() == kFillWithConstant;
}
inline void StateVariableSpecification::set_has_fill_with_constant() {
  _oneof_case_[0] = kFillWithConstant;
}
inline void StateVariableSpecification::clear_fill_with_constant() {
  if (has_fill_with_constant()) {
    delete usage_.fill_with_constant_;
    clear_has_usage();
  }
}
inline const ::tensorflow::tpu::StateVariableSpecification_FillWithConstant& StateVariableSpecification::_internal_fill_with_constant() const {
  return *usage_.fill_with_constant_;
}
inline ::tensorflow::tpu::StateVariableSpecification_FillWithConstant* StateVariableSpecification::release_fill_with_constant() {
  // @@protoc_insertion_point(field_release:tensorflow.tpu.StateVariableSpecification.fill_with_constant)
  if (has_fill_with_constant()) {
    clear_has_usage();
      ::tensorflow::tpu::StateVariableSpecification_FillWithConstant* temp = usage_.fill_with_constant_;
    usage_.fill_with_constant_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline const ::tensorflow::tpu::StateVariableSpecification_FillWithConstant& StateVariableSpecification::fill_with_constant() const {
  // @@protoc_insertion_point(field_get:tensorflow.tpu.StateVariableSpecification.fill_with_constant)
  return has_fill_with_constant()
      ? *usage_.fill_with_constant_
      : *reinterpret_cast< ::tensorflow::tpu::StateVariableSpecification_FillWithConstant*>(&::tensorflow::tpu::_StateVariableSpecification_FillWithConstant_default_instance_);
}
inline ::tensorflow::tpu::StateVariableSpecification_FillWithConstant* StateVariableSpecification::mutable_fill_with_constant() {
  if (!has_fill_with_constant()) {
    clear_usage();
    set_has_fill_with_constant();
    usage_.fill_with_constant_ = CreateMaybeMessage< ::tensorflow::tpu::StateVariableSpecification_FillWithConstant >(
        GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.tpu.StateVariableSpecification.fill_with_constant)
  return usage_.fill_with_constant_;
}

inline bool StateVariableSpecification::has_usage() const {
  return usage_case() != USAGE_NOT_SET;
}
inline void StateVariableSpecification::clear_has_usage() {
  _oneof_case_[0] = USAGE_NOT_SET;
}
inline StateVariableSpecification::UsageCase StateVariableSpecification::usage_case() const {
  return StateVariableSpecification::UsageCase(_oneof_case_[0]);
}
#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace tpu
}  // namespace tensorflow

namespace google {
namespace protobuf {

template <> struct is_proto_enum< ::tensorflow::tpu::GradientAccumulationStatus_Status> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::tensorflow::tpu::GradientAccumulationStatus_Status>() {
  return ::tensorflow::tpu::GradientAccumulationStatus_Status_descriptor();
}
template <> struct is_proto_enum< ::tensorflow::tpu::LowDimensionalPackingStatus_Status> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::tensorflow::tpu::LowDimensionalPackingStatus_Status>() {
  return ::tensorflow::tpu::LowDimensionalPackingStatus_Status_descriptor();
}
template <> struct is_proto_enum< ::tensorflow::tpu::HotIdReplicationConfiguration_Status> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::tensorflow::tpu::HotIdReplicationConfiguration_Status>() {
  return ::tensorflow::tpu::HotIdReplicationConfiguration_Status_descriptor();
}

}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_INCLUDED_tensorflow_2fcore_2fprotobuf_2ftpu_2foptimization_5fparameters_2eproto
