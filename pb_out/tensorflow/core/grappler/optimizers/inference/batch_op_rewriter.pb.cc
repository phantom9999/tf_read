// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/grappler/optimizers/inference/batch_op_rewriter.proto

#include "tensorflow/core/grappler/optimizers/inference/batch_op_rewriter.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// This is a temporary google only hack
#ifdef GOOGLE_PROTOBUF_ENFORCE_UNIQUENESS
#include "third_party/protobuf/version.h"
#endif
// @@protoc_insertion_point(includes)

namespace protobuf_google_2fprotobuf_2fwrappers_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_google_2fprotobuf_2fwrappers_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_UInt32Value;
}  // namespace protobuf_google_2fprotobuf_2fwrappers_2eproto
namespace protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse;
}  // namespace protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto
namespace tensorflow {
namespace serving {
class BatchOpRewriteConfig_AdaptiveBatchSchedulerOptionDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BatchOpRewriteConfig_AdaptiveBatchSchedulerOption>
      _instance;
} _BatchOpRewriteConfig_AdaptiveBatchSchedulerOption_default_instance_;
class BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse>
      _instance;
} _BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse_default_instance_;
class BatchOpRewriteConfigDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BatchOpRewriteConfig>
      _instance;
} _BatchOpRewriteConfig_default_instance_;
}  // namespace serving
}  // namespace tensorflow
namespace protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto {
static void InitDefaultsBatchOpRewriteConfig_AdaptiveBatchSchedulerOption() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption_default_instance_;
    new (ptr) ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsBatchOpRewriteConfig_AdaptiveBatchSchedulerOption}, {
      &protobuf_google_2fprotobuf_2fwrappers_2eproto::scc_info_UInt32Value.base,}};

static void InitDefaultsBatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse_default_instance_;
    new (ptr) ::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse();
  }
  ::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsBatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse}, {
      &protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::scc_info_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption.base,}};

static void InitDefaultsBatchOpRewriteConfig() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_BatchOpRewriteConfig_default_instance_;
    new (ptr) ::tensorflow::serving::BatchOpRewriteConfig();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::BatchOpRewriteConfig::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_BatchOpRewriteConfig =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsBatchOpRewriteConfig}, {
      &protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::scc_info_BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse.base,}};

void InitDefaults() {
  ::google::protobuf::internal::InitSCC(&scc_info_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BatchOpRewriteConfig.base);
}

::google::protobuf::Metadata file_level_metadata[3];

const ::google::protobuf::uint32 TableStruct::offsets[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption, min_inflight_batches_limit_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption, initial_inflight_batches_limit_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption, max_inflight_batches_limit_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption, batches_to_average_over_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse, key_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig, enable_adaptive_shared_batching_thread_pool_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::tensorflow::serving::BatchOpRewriteConfig, model_scheduler_options_),
};
static const ::google::protobuf::internal::MigrationSchema schemas[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, sizeof(::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption)},
  { 9, 16, sizeof(::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse)},
  { 18, -1, sizeof(::tensorflow::serving::BatchOpRewriteConfig)},
};

static ::google::protobuf::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_BatchOpRewriteConfig_default_instance_),
};

void protobuf_AssignDescriptors() {
  AddDescriptors();
  AssignDescriptors(
      "tensorflow/core/grappler/optimizers/inference/batch_op_rewriter.proto", schemas, file_default_instances, TableStruct::offsets,
      file_level_metadata, NULL, NULL);
}

void protobuf_AssignDescriptorsOnce() {
  static ::google::protobuf::internal::once_flag once;
  ::google::protobuf::internal::call_once(once, protobuf_AssignDescriptors);
}

void protobuf_RegisterTypes(const ::std::string&) GOOGLE_PROTOBUF_ATTRIBUTE_COLD;
void protobuf_RegisterTypes(const ::std::string&) {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::internal::RegisterAllTypes(file_level_metadata, 3);
}

void AddDescriptorsImpl() {
  InitDefaults();
  static const char descriptor[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
      "\nEtensorflow/core/grappler/optimizers/in"
      "ference/batch_op_rewriter.proto\022\022tensorf"
      "low.serving\032\036google/protobuf/wrappers.pr"
      "oto\"\341\004\n\024BatchOpRewriteConfig\0223\n+enable_a"
      "daptive_shared_batching_thread_pool\030\004 \001("
      "\010\022d\n\027model_scheduler_options\030\001 \003(\0132C.ten"
      "sorflow.serving.BatchOpRewriteConfig.Mod"
      "elSchedulerOptionsEntry\032\247\002\n\034AdaptiveBatc"
      "hSchedulerOption\022@\n\032min_inflight_batches"
      "_limit\030\001 \001(\0132\034.google.protobuf.UInt32Val"
      "ue\022D\n\036initial_inflight_batches_limit\030\002 \001"
      "(\0132\034.google.protobuf.UInt32Value\022@\n\032max_"
      "inflight_batches_limit\030\003 \001(\0132\034.google.pr"
      "otobuf.UInt32Value\022=\n\027batches_to_average"
      "_over\030\004 \001(\0132\034.google.protobuf.UInt32Valu"
      "e\032\203\001\n\032ModelSchedulerOptionsEntry\022\013\n\003key\030"
      "\001 \001(\t\022T\n\005value\030\002 \001(\0132E.tensorflow.servin"
      "g.BatchOpRewriteConfig.AdaptiveBatchSche"
      "dulerOption:\0028\001b\006proto3"
  };
  ::google::protobuf::DescriptorPool::InternalAddGeneratedFile(
      descriptor, 743);
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedFile(
    "tensorflow/core/grappler/optimizers/inference/batch_op_rewriter.proto", &protobuf_RegisterTypes);
  ::protobuf_google_2fprotobuf_2fwrappers_2eproto::AddDescriptors();
}

void AddDescriptors() {
  static ::google::protobuf::internal::once_flag once;
  ::google::protobuf::internal::call_once(once, AddDescriptorsImpl);
}
// Force AddDescriptors() to be called at dynamic initialization time.
struct StaticDescriptorInitializer {
  StaticDescriptorInitializer() {
    AddDescriptors();
  }
} static_descriptor_initializer;
}  // namespace protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto
namespace tensorflow {
namespace serving {

// ===================================================================

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::InitAsDefaultInstance() {
  ::tensorflow::serving::_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption_default_instance_._instance.get_mutable()->min_inflight_batches_limit_ = const_cast< ::google::protobuf::UInt32Value*>(
      ::google::protobuf::UInt32Value::internal_default_instance());
  ::tensorflow::serving::_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption_default_instance_._instance.get_mutable()->initial_inflight_batches_limit_ = const_cast< ::google::protobuf::UInt32Value*>(
      ::google::protobuf::UInt32Value::internal_default_instance());
  ::tensorflow::serving::_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption_default_instance_._instance.get_mutable()->max_inflight_batches_limit_ = const_cast< ::google::protobuf::UInt32Value*>(
      ::google::protobuf::UInt32Value::internal_default_instance());
  ::tensorflow::serving::_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption_default_instance_._instance.get_mutable()->batches_to_average_over_ = const_cast< ::google::protobuf::UInt32Value*>(
      ::google::protobuf::UInt32Value::internal_default_instance());
}
void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::clear_min_inflight_batches_limit() {
  if (GetArenaNoVirtual() == NULL && min_inflight_batches_limit_ != NULL) {
    delete min_inflight_batches_limit_;
  }
  min_inflight_batches_limit_ = NULL;
}
void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::clear_initial_inflight_batches_limit() {
  if (GetArenaNoVirtual() == NULL && initial_inflight_batches_limit_ != NULL) {
    delete initial_inflight_batches_limit_;
  }
  initial_inflight_batches_limit_ = NULL;
}
void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::clear_max_inflight_batches_limit() {
  if (GetArenaNoVirtual() == NULL && max_inflight_batches_limit_ != NULL) {
    delete max_inflight_batches_limit_;
  }
  max_inflight_batches_limit_ = NULL;
}
void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::clear_batches_to_average_over() {
  if (GetArenaNoVirtual() == NULL && batches_to_average_over_ != NULL) {
    delete batches_to_average_over_;
  }
  batches_to_average_over_ = NULL;
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::kMinInflightBatchesLimitFieldNumber;
const int BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::kInitialInflightBatchesLimitFieldNumber;
const int BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::kMaxInflightBatchesLimitFieldNumber;
const int BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::kBatchesToAverageOverFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::scc_info_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
}
BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption(const BatchOpRewriteConfig_AdaptiveBatchSchedulerOption& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_min_inflight_batches_limit()) {
    min_inflight_batches_limit_ = new ::google::protobuf::UInt32Value(*from.min_inflight_batches_limit_);
  } else {
    min_inflight_batches_limit_ = NULL;
  }
  if (from.has_initial_inflight_batches_limit()) {
    initial_inflight_batches_limit_ = new ::google::protobuf::UInt32Value(*from.initial_inflight_batches_limit_);
  } else {
    initial_inflight_batches_limit_ = NULL;
  }
  if (from.has_max_inflight_batches_limit()) {
    max_inflight_batches_limit_ = new ::google::protobuf::UInt32Value(*from.max_inflight_batches_limit_);
  } else {
    max_inflight_batches_limit_ = NULL;
  }
  if (from.has_batches_to_average_over()) {
    batches_to_average_over_ = new ::google::protobuf::UInt32Value(*from.batches_to_average_over_);
  } else {
    batches_to_average_over_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::SharedCtor() {
  ::memset(&min_inflight_batches_limit_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&batches_to_average_over_) -
      reinterpret_cast<char*>(&min_inflight_batches_limit_)) + sizeof(batches_to_average_over_));
}

BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::~BatchOpRewriteConfig_AdaptiveBatchSchedulerOption() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  SharedDtor();
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::SharedDtor() {
  if (this != internal_default_instance()) delete min_inflight_batches_limit_;
  if (this != internal_default_instance()) delete initial_inflight_batches_limit_;
  if (this != internal_default_instance()) delete max_inflight_batches_limit_;
  if (this != internal_default_instance()) delete batches_to_average_over_;
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::descriptor() {
  ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BatchOpRewriteConfig_AdaptiveBatchSchedulerOption& BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::scc_info_BatchOpRewriteConfig_AdaptiveBatchSchedulerOption.base);
  return *internal_default_instance();
}


void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaNoVirtual() == NULL && min_inflight_batches_limit_ != NULL) {
    delete min_inflight_batches_limit_;
  }
  min_inflight_batches_limit_ = NULL;
  if (GetArenaNoVirtual() == NULL && initial_inflight_batches_limit_ != NULL) {
    delete initial_inflight_batches_limit_;
  }
  initial_inflight_batches_limit_ = NULL;
  if (GetArenaNoVirtual() == NULL && max_inflight_batches_limit_ != NULL) {
    delete max_inflight_batches_limit_;
  }
  max_inflight_batches_limit_ = NULL;
  if (GetArenaNoVirtual() == NULL && batches_to_average_over_ != NULL) {
    delete batches_to_average_over_;
  }
  batches_to_average_over_ = NULL;
  _internal_metadata_.Clear();
}

bool BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .google.protobuf.UInt32Value min_inflight_batches_limit = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_min_inflight_batches_limit()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .google.protobuf.UInt32Value initial_inflight_batches_limit = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_initial_inflight_batches_limit()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .google.protobuf.UInt32Value max_inflight_batches_limit = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_max_inflight_batches_limit()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .google.protobuf.UInt32Value batches_to_average_over = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_batches_to_average_over()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  return false;
#undef DO_
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .google.protobuf.UInt32Value min_inflight_batches_limit = 1;
  if (this->has_min_inflight_batches_limit()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_min_inflight_batches_limit(), output);
  }

  // .google.protobuf.UInt32Value initial_inflight_batches_limit = 2;
  if (this->has_initial_inflight_batches_limit()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, this->_internal_initial_inflight_batches_limit(), output);
  }

  // .google.protobuf.UInt32Value max_inflight_batches_limit = 3;
  if (this->has_max_inflight_batches_limit()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, this->_internal_max_inflight_batches_limit(), output);
  }

  // .google.protobuf.UInt32Value batches_to_average_over = 4;
  if (this->has_batches_to_average_over()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      4, this->_internal_batches_to_average_over(), output);
  }

  if ((_internal_metadata_.have_unknown_fields() &&  ::google::protobuf::internal::GetProto3PreserveUnknownsDefault())) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        (::google::protobuf::internal::GetProto3PreserveUnknownsDefault()   ? _internal_metadata_.unknown_fields()   : _internal_metadata_.default_instance()), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
}

::google::protobuf::uint8* BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .google.protobuf.UInt32Value min_inflight_batches_limit = 1;
  if (this->has_min_inflight_batches_limit()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_min_inflight_batches_limit(), deterministic, target);
  }

  // .google.protobuf.UInt32Value initial_inflight_batches_limit = 2;
  if (this->has_initial_inflight_batches_limit()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->_internal_initial_inflight_batches_limit(), deterministic, target);
  }

  // .google.protobuf.UInt32Value max_inflight_batches_limit = 3;
  if (this->has_max_inflight_batches_limit()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->_internal_max_inflight_batches_limit(), deterministic, target);
  }

  // .google.protobuf.UInt32Value batches_to_average_over = 4;
  if (this->has_batches_to_average_over()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        4, this->_internal_batches_to_average_over(), deterministic, target);
  }

  if ((_internal_metadata_.have_unknown_fields() &&  ::google::protobuf::internal::GetProto3PreserveUnknownsDefault())) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        (::google::protobuf::internal::GetProto3PreserveUnknownsDefault()   ? _internal_metadata_.unknown_fields()   : _internal_metadata_.default_instance()), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  return target;
}

size_t BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  size_t total_size = 0;

  if ((_internal_metadata_.have_unknown_fields() &&  ::google::protobuf::internal::GetProto3PreserveUnknownsDefault())) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        (::google::protobuf::internal::GetProto3PreserveUnknownsDefault()   ? _internal_metadata_.unknown_fields()   : _internal_metadata_.default_instance()));
  }
  // .google.protobuf.UInt32Value min_inflight_batches_limit = 1;
  if (this->has_min_inflight_batches_limit()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *min_inflight_batches_limit_);
  }

  // .google.protobuf.UInt32Value initial_inflight_batches_limit = 2;
  if (this->has_initial_inflight_batches_limit()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *initial_inflight_batches_limit_);
  }

  // .google.protobuf.UInt32Value max_inflight_batches_limit = 3;
  if (this->has_max_inflight_batches_limit()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *max_inflight_batches_limit_);
  }

  // .google.protobuf.UInt32Value batches_to_average_over = 4;
  if (this->has_batches_to_average_over()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *batches_to_average_over_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  GOOGLE_DCHECK_NE(&from, this);
  const BatchOpRewriteConfig_AdaptiveBatchSchedulerOption* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BatchOpRewriteConfig_AdaptiveBatchSchedulerOption>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
    MergeFrom(*source);
  }
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::MergeFrom(const BatchOpRewriteConfig_AdaptiveBatchSchedulerOption& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_min_inflight_batches_limit()) {
    mutable_min_inflight_batches_limit()->::google::protobuf::UInt32Value::MergeFrom(from.min_inflight_batches_limit());
  }
  if (from.has_initial_inflight_batches_limit()) {
    mutable_initial_inflight_batches_limit()->::google::protobuf::UInt32Value::MergeFrom(from.initial_inflight_batches_limit());
  }
  if (from.has_max_inflight_batches_limit()) {
    mutable_max_inflight_batches_limit()->::google::protobuf::UInt32Value::MergeFrom(from.max_inflight_batches_limit());
  }
  if (from.has_batches_to_average_over()) {
    mutable_batches_to_average_over()->::google::protobuf::UInt32Value::MergeFrom(from.batches_to_average_over());
  }
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::CopyFrom(const BatchOpRewriteConfig_AdaptiveBatchSchedulerOption& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::IsInitialized() const {
  return true;
}

void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::Swap(BatchOpRewriteConfig_AdaptiveBatchSchedulerOption* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::InternalSwap(BatchOpRewriteConfig_AdaptiveBatchSchedulerOption* other) {
  using std::swap;
  swap(min_inflight_batches_limit_, other->min_inflight_batches_limit_);
  swap(initial_inflight_batches_limit_, other->initial_inflight_batches_limit_);
  swap(max_inflight_batches_limit_, other->max_inflight_batches_limit_);
  swap(batches_to_average_over_, other->batches_to_average_over_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BatchOpRewriteConfig_AdaptiveBatchSchedulerOption::GetMetadata() const {
  protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse() {}
BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse(::google::protobuf::Arena* arena) : SuperType(arena) {}
void BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse::MergeFrom(const BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::google::protobuf::Metadata BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse::GetMetadata() const {
  ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::file_level_metadata[1];
}
void BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse::MergeFrom(
    const ::google::protobuf::Message& other) {
  ::google::protobuf::Message::MergeFrom(other);
}


// ===================================================================

void BatchOpRewriteConfig::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BatchOpRewriteConfig::kEnableAdaptiveSharedBatchingThreadPoolFieldNumber;
const int BatchOpRewriteConfig::kModelSchedulerOptionsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BatchOpRewriteConfig::BatchOpRewriteConfig()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::scc_info_BatchOpRewriteConfig.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.BatchOpRewriteConfig)
}
BatchOpRewriteConfig::BatchOpRewriteConfig(const BatchOpRewriteConfig& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  model_scheduler_options_.MergeFrom(from.model_scheduler_options_);
  enable_adaptive_shared_batching_thread_pool_ = from.enable_adaptive_shared_batching_thread_pool_;
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.BatchOpRewriteConfig)
}

void BatchOpRewriteConfig::SharedCtor() {
  enable_adaptive_shared_batching_thread_pool_ = false;
}

BatchOpRewriteConfig::~BatchOpRewriteConfig() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.BatchOpRewriteConfig)
  SharedDtor();
}

void BatchOpRewriteConfig::SharedDtor() {
}

void BatchOpRewriteConfig::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BatchOpRewriteConfig::descriptor() {
  ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BatchOpRewriteConfig& BatchOpRewriteConfig::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::scc_info_BatchOpRewriteConfig.base);
  return *internal_default_instance();
}


void BatchOpRewriteConfig::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.BatchOpRewriteConfig)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  model_scheduler_options_.Clear();
  enable_adaptive_shared_batching_thread_pool_ = false;
  _internal_metadata_.Clear();
}

bool BatchOpRewriteConfig::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.BatchOpRewriteConfig)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // map<string, .tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption> model_scheduler_options = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse::Parser< ::google::protobuf::internal::MapField<
              BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse,
              ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_MESSAGE,
              0 >,
            ::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption > > parser(&model_scheduler_options_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), static_cast<int>(parser.key().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.BatchOpRewriteConfig.ModelSchedulerOptionsEntry.key"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // bool enable_adaptive_shared_batching_thread_pool = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u /* 32 & 0xFF */)) {

          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &enable_adaptive_shared_batching_thread_pool_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.BatchOpRewriteConfig)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.BatchOpRewriteConfig)
  return false;
#undef DO_
}

void BatchOpRewriteConfig::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.BatchOpRewriteConfig)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // map<string, .tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption> model_scheduler_options = 1;
  if (!this->model_scheduler_options().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.BatchOpRewriteConfig.ModelSchedulerOptionsEntry.key");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->model_scheduler_options().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->model_scheduler_options().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::const_iterator
          it = this->model_scheduler_options().begin();
          it != this->model_scheduler_options().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(model_scheduler_options_.NewEntryWrapper(
            items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
            1, *entry, output);
        Utf8Check::Check(items[static_cast<ptrdiff_t>(i)]);
      }
    } else {
      ::std::unique_ptr<BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::const_iterator
          it = this->model_scheduler_options().begin();
          it != this->model_scheduler_options().end(); ++it) {
        entry.reset(model_scheduler_options_.NewEntryWrapper(
            it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
            1, *entry, output);
        Utf8Check::Check(&*it);
      }
    }
  }

  // bool enable_adaptive_shared_batching_thread_pool = 4;
  if (this->enable_adaptive_shared_batching_thread_pool() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(4, this->enable_adaptive_shared_batching_thread_pool(), output);
  }

  if ((_internal_metadata_.have_unknown_fields() &&  ::google::protobuf::internal::GetProto3PreserveUnknownsDefault())) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        (::google::protobuf::internal::GetProto3PreserveUnknownsDefault()   ? _internal_metadata_.unknown_fields()   : _internal_metadata_.default_instance()), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.BatchOpRewriteConfig)
}

::google::protobuf::uint8* BatchOpRewriteConfig::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.BatchOpRewriteConfig)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // map<string, .tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption> model_scheduler_options = 1;
  if (!this->model_scheduler_options().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "tensorflow.serving.BatchOpRewriteConfig.ModelSchedulerOptionsEntry.key");
      }
    };

    if (deterministic &&
        this->model_scheduler_options().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->model_scheduler_options().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::const_iterator
          it = this->model_scheduler_options().begin();
          it != this->model_scheduler_options().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(model_scheduler_options_.NewEntryWrapper(
            items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        target = ::google::protobuf::internal::WireFormatLite::
                   InternalWriteMessageNoVirtualToArray(
                       1, *entry, deterministic, target);
;
        Utf8Check::Check(items[static_cast<ptrdiff_t>(i)]);
      }
    } else {
      ::std::unique_ptr<BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::const_iterator
          it = this->model_scheduler_options().begin();
          it != this->model_scheduler_options().end(); ++it) {
        entry.reset(model_scheduler_options_.NewEntryWrapper(
            it->first, it->second));
        target = ::google::protobuf::internal::WireFormatLite::
                   InternalWriteMessageNoVirtualToArray(
                       1, *entry, deterministic, target);
;
        Utf8Check::Check(&*it);
      }
    }
  }

  // bool enable_adaptive_shared_batching_thread_pool = 4;
  if (this->enable_adaptive_shared_batching_thread_pool() != 0) {
    target = ::google::protobuf::internal::WireFormatLite::WriteBoolToArray(4, this->enable_adaptive_shared_batching_thread_pool(), target);
  }

  if ((_internal_metadata_.have_unknown_fields() &&  ::google::protobuf::internal::GetProto3PreserveUnknownsDefault())) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        (::google::protobuf::internal::GetProto3PreserveUnknownsDefault()   ? _internal_metadata_.unknown_fields()   : _internal_metadata_.default_instance()), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.BatchOpRewriteConfig)
  return target;
}

size_t BatchOpRewriteConfig::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.BatchOpRewriteConfig)
  size_t total_size = 0;

  if ((_internal_metadata_.have_unknown_fields() &&  ::google::protobuf::internal::GetProto3PreserveUnknownsDefault())) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        (::google::protobuf::internal::GetProto3PreserveUnknownsDefault()   ? _internal_metadata_.unknown_fields()   : _internal_metadata_.default_instance()));
  }
  // map<string, .tensorflow.serving.BatchOpRewriteConfig.AdaptiveBatchSchedulerOption> model_scheduler_options = 1;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->model_scheduler_options_size());
  {
    ::std::unique_ptr<BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse> entry;
    for (::google::protobuf::Map< ::std::string, ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >::const_iterator
        it = this->model_scheduler_options().begin();
        it != this->model_scheduler_options().end(); ++it) {
      entry.reset(model_scheduler_options_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
  }

  // bool enable_adaptive_shared_batching_thread_pool = 4;
  if (this->enable_adaptive_shared_batching_thread_pool() != 0) {
    total_size += 1 + 1;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BatchOpRewriteConfig::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.BatchOpRewriteConfig)
  GOOGLE_DCHECK_NE(&from, this);
  const BatchOpRewriteConfig* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BatchOpRewriteConfig>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.BatchOpRewriteConfig)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.BatchOpRewriteConfig)
    MergeFrom(*source);
  }
}

void BatchOpRewriteConfig::MergeFrom(const BatchOpRewriteConfig& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.BatchOpRewriteConfig)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  model_scheduler_options_.MergeFrom(from.model_scheduler_options_);
  if (from.enable_adaptive_shared_batching_thread_pool() != 0) {
    set_enable_adaptive_shared_batching_thread_pool(from.enable_adaptive_shared_batching_thread_pool());
  }
}

void BatchOpRewriteConfig::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.BatchOpRewriteConfig)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BatchOpRewriteConfig::CopyFrom(const BatchOpRewriteConfig& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.BatchOpRewriteConfig)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BatchOpRewriteConfig::IsInitialized() const {
  return true;
}

void BatchOpRewriteConfig::Swap(BatchOpRewriteConfig* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BatchOpRewriteConfig::InternalSwap(BatchOpRewriteConfig* other) {
  using std::swap;
  model_scheduler_options_.Swap(&other->model_scheduler_options_);
  swap(enable_adaptive_shared_batching_thread_pool_, other->enable_adaptive_shared_batching_thread_pool_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BatchOpRewriteConfig::GetMetadata() const {
  protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_tensorflow_2fcore_2fgrappler_2foptimizers_2finference_2fbatch_5fop_5frewriter_2eproto::file_level_metadata[kIndexInFileMessages];
}


// @@protoc_insertion_point(namespace_scope)
}  // namespace serving
}  // namespace tensorflow
namespace google {
namespace protobuf {
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption* Arena::CreateMaybeMessage< ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >(Arena* arena) {
  return Arena::CreateInternal< ::tensorflow::serving::BatchOpRewriteConfig_AdaptiveBatchSchedulerOption >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse* Arena::CreateMaybeMessage< ::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateInternal< ::tensorflow::serving::BatchOpRewriteConfig_ModelSchedulerOptionsEntry_DoNotUse >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::tensorflow::serving::BatchOpRewriteConfig* Arena::CreateMaybeMessage< ::tensorflow::serving::BatchOpRewriteConfig >(Arena* arena) {
  return Arena::CreateInternal< ::tensorflow::serving::BatchOpRewriteConfig >(arena);
}
}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)
